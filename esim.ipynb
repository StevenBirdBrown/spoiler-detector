{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Enhanced Sequence Inference Model\n","The model is based on [this paper](https://arxiv.org/abs/1609.06038).\n","\n","### Import Related Packages"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":"import os\nimport json\nimport zipfile\nimport time\nimport itertools\n\nimport numpy as np\nimport mxnet as mx\nimport multiprocessing as mp\nimport gluonnlp as nlp\n\nfrom mxnet import gluon, nd, init\nfrom mxnet.gluon import nn, rnn\nfrom mxnet import autograd, gluon, nd\nfrom d2l import try_gpu\nimport pandas as pd\n\n# sklearn's metric function to evaluate the results of the experiment\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# fixed random number seed\nnp.random.seed(9102)\nmx.random.seed(9102)"},{"cell_type":"markdown","metadata":{},"source":["## Data pipeline\n","\n","### Load Dataset\n","\n","See [dataloader](data_loader.ipynb) for how the training samples are obtained."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":"(1000, 4)"},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":"data_folder = 'data/imdb/'\nfile_name = 'train.csv'\nfile_path = data_folder + file_name\n# dev: True - only use a small dataset\n# create_vocab: True - create a new vocabulary from training data\ndev = True\n# load train file\nif dev:\n    # load only n rows\n    nrows = 1000\n    data = pd.read_csv(file_path, nrows=nrows)\nelse:\n    # load as many as possible\n    data = pd.read_csv(file_path)\ndata.shape"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":"((1000, 4),     movie_id                                          sentence1  \\\n 0  tt0097576  The story opens in Monument Valley, Utah, in 1...   \n 1  tt2488496  Luke Skywalker has vanished. In his absence, t...   \n 2  tt0407887  In voiceover, Irish-American mobster Frank Cos...   \n 3  tt0209144  This is a complex story about Leonard Shelby (...   \n 4  tt1285016  In October 2003, Harvard University student Ma...   \n \n                                            sentence2  label  \n 0  Chemistry between Ford and Connery puts this f...  False  \n 1  You did it, J.J. ... You have ruined Star Wars...   True  \n 2  No respect! This remake of \"Internal Affairs\",...  False  \n 3  Scientific and psychological thriller. Who kne...   True  \n 4  A Grossly Overrated But Still Very Good Film A...  False  )"},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":"data.dropna(inplace=True)\ndata.shape, data.head()"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"label2num = {'contradiction':0, 'neutral':1, 'entailment':2}"},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":"(900, 100)"},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# create a list of review a label paris.\n","dataset = [[left, right, label] for left, right, label in \\\n","           zip(data['sentence1'], data['sentence2'], data['label']) if label!='-' ]\n","# randomly divide one percent from the training set as a verification set.\n","train_dataset, valid_dataset = nlp.data.train_valid_split(dataset, valid_ratio=.1)\n","len(train_dataset), len(valid_dataset)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/plain":"icle catches Zero\\'s eye. He rushes back to the hotel to show Gustave, that Madame D. has been found dead in her bathroom. A shocked Gustave immediately takes Zero on a train where they travel to Madame D.\\'s estate.On the train, they stop by a barley field on a day that is known as the \"Closing of the Frontier\". Gustave and Zero see soldiers standing in the field. A group of soldiers enter and ask to see the documents of both men. Gustave shows his papers, but Zero has none as he is an immigrant. The main soldier orders Zero to come with him. Gustave defends Zero and gets into a fight with the soldiers. They pin the two of them against the wall, and Gustave, seeing Zero in trouble, barks at the soldiers, \"TAKE YOUR HANDS OFF MY LOBBY BOY!\"The fracas soon catches the attention of Inspector Henckels (Edward Norton), who enters the car, and quickly recognizes Gustave! When Gustave hears the names of Henckel\\'s parents, he quickly remembers who they were, as well as Henckel\\'s childhood name of \"Little Albert\". Henckels gives Zero a pardon for the trip, but urges him to get official papers immediately.Gustave and Zero soon after, arrive at Madame D.\\'s mansion. Her maid, Clotilde (Lea Seydoux), guides them to the old woman\\'s body lying in a casket. Gustave speaks to her corpse, praising her as if she were still alive.Eventually, Clotilde tells Gustave that the butler, M. Serge X (Mathieu Amalric), wishes to speak to him. However, Serge is frantic and panicky, and quickly rushes off, with Gustave and Zero in pursuit.They soon find themselves in the trophy room of the house, where all manner of relations to Madame D are present for the reading of her will. Surprising to both Gustave and Zero, is Mr Kovacs, who is the executor of her estate, and who has analyzed her will, and its numerous amendments.Key among the inheritors mentioned, are her son Dmitri (Adrien Brody) and his three sisters, Marguerite (Michaela Caspar), Laetizia (Sabine Urig), and Carolina (Heike Hanold-Lynch). There are also small provisions for other members of the family, but Kovacs claims that a new amendment was sent to his offices just recently. Reading it aloud, the amendment (which is still being investigated), thanks Gustave for his kindness, and allows him ownership of cherished painting, titled \"Boy With Apple.\"When Gustave steps forward, Dmitri angrily confronts him, hurling several homophobic slurs, and refusing him the painting. A small war of words causes a small scuffle among Dmitri, Gustave, Zero, and Dmitri\\'s right-hand man, J.G. Jopling (Willem Dafoe).Gustave then takes Zero to the room where \"Boy With Apple\" is hanging. After admiring it for a little while, the two take it down, and replace it (with a rather vulgar painting). Gustave then has Serge wrap up \"Boy With Apple\". Unseen by anyone else, Serge tucks an envelope labeled \"CONFIDENTIAL\" into the rear of the painting, before it is wrapped in brown paper, and given to Gustave. Before Gustave and Zero leave, Gustave remembers that Serge had wanted to speak to him before about something, but the Frenchman does not say anything more.On the train ride back, Gustave claims he will cherish the gift from Madame D, but quickly realizes that Dmitri may come looking for the painting. He proposes to Zero that they sell off \"Boy With Apple\" on the black market, and, in exchange for his services, Zero will be given 1.5% of the cut and Gustave will make him his heir. Zero agrees, and quickly jots down the concierge\\'s declaration, to make it \"official.\"Once back at the hotel, the two hide \"Boy With Apple\" in the hotel\\'s vault, before Henckels arrives, to arrest Gustave as the suspect murderer in Madame D\\'s death. As Zero watches, Gustave attempts to run, but Henckels and his men give chase.\\nPart 3 - Check Point 19 - Criminal-Interment CampOne week before his trial, Gustave is imprisoned. Going to visit him, Zero is shocked to find his senior has suffered injuries, but Gustave simply claims he had to prove himself to the others, once they questioned his virility.Zero has also met with Mr Kovacs, who explains that a deposition was given by several members of Madame D\\'s family, that Gustave had secretly entered the mansion, and poisoned Madame D with strychnine. However, the one who claimed to have seen the events unfold, Serge X, has gone missing.Gustave is pretty sure that Madame\\'s family forced Serge to bear false witness, and though Gustave has an alibi as to where he was at the time, he fears bringing the woman to whose company he was in (the Duchess of Westphalia) into the events, as it will ruin her reputation.In the meantime, Dmitri has secretly begun his own search for the missing Serge, sending his henchman Jopling to check on Serge\\'s sister, who claims she has not seen her brother recently.Zero soon ends up acting as the middle-man for Gustave\\'s correspondence with the staff of the Grand Budapest, reading them the concierge\\'s letters, and his own prose poems. Gustave also requests that any issues should be addressed to Zero in his absence.Back in the prison, Gustave has almost become a concierge to the inmates there, serving many of them mush with a cheery air. He has also shared the Mendl\\'s pastries he receives, with several inmates: Pinky (Florian Lukas), Wolf (Karl Markovics), Gunther (Volker Michalowski), and Ludwig (Harvey Keitel). Gustave\\'s hospitality towards the men pays off, and they want to help him break out of the prison. However, the amount of tools to break out of the prison are limited, but upon looking at the Mendl\\'s box, Gustave hatches a cunning plan.It is at this point where Old Zero becomes overwhelmed with emotions and he starts to cry. He explains to the Author that talking about Agatha makes him emotional. Old Zero then stops his main story, and gives some background on his and Agatha\\'s relationship, of which even Gustave was privy to (though seemed to also flirt with, much to Zero\\'s ire), even \\'interviewing\\' the young woman to see if she was proper for his Lobby Boy.Returning to the story, Zero worked with Agatha, to place specific tools, baked into several pastries sent to Gustave. This fashioning of them to look like baked goods fooled the prison guards, and allowed Gustave and his comrades the proper tools to attempt their escape.In regards to Madame D\\'s will, Kovacs is of the mind that something is missing from the paperwork, and that along with the disappearance of Serge, the executor asks Dmitri and his sisters to bring the local authorities to look into the matter...of which Dmitri quickly refuses to do so. Dmitri simply seems to want what he feels is owed him, while Kovacs is of the mind that he must follow the instructions his former client laid out, and proceed in an honest matter. This retort is met with Dmitri storming out of the room, and Jopling throwing Kovac\\'s cat out the window to its death.Later, Kovacs collects the cat\\'s body and boards a trolley, but soon finds Jopling following him. Kovacs attempts to lose the bodyguard, by ducking into the Kunst Museum (after quickly disposing of his dead cat in a trash can). However, as he attempts to leave out a rear door, Jopling stops him, slamming the door, severing 4 of the man\\'s fingers, before murdering him. Kovac\\'s absence is felt the next day at The Grand Budapest, when a note is received from his office, telling that his usual visit has been cancelled.As the time for Gustave\\'s escape draws near, Zero tells Agatha about \"Boy With Apple.\" Fearful that he and Gustave might be caught, he gives her the necessary information to remove it from the hotel\\'s vault (information which she reluctantly takes).At the prison, Gustave and his fellow inmates begin to put their plan of escape into action. Aside from a noisy prisoner who sees them escape (who is quelled by an inmate whom Gustave gave mush to), the group finds their biggest obstacle in an underground hatch, which is occupied with several guards playing poker. Gunther sacrifices himself for the group, killing the guards, but dying in the process.Finally, the remaining men reach the outside of the prison with Zero waiting for Gustave. Ludwig, Pinky, and Wolf part ways, overtaking a nearby bus. However, Gustave soon grows upset at Zero when he finds the lobby boy has not procured a safe house, spare clothes, or brought his favorite cologne. Gustave then angrily lashes out at the boy, criticizing his culture, before the boy shames the concierge, by telling how his family was killed, forcing him to retreat and look for work on his own. Gustave sincerely apologizes to Zero, before the escape sirens blare, and the two make a run for it.In the aftermath of the escape Henckels and his men investigate the break out, but also find Jopling in their midst. Henckels also informs Jopling that Madame D\\'s lawyer was found dead just recently. Jopling claims he was aware Mr Kovacs had gone missing, but claims he knew nothing of the man\\'s death, before being escorted from the prison.After traipsing across the snow-covered countryside, Gustave and Zero find a telephone box. Once he gets through, Gustave then relays a special request to...Part 4 - The Society of the Crossed KeysThe society turns out to be an inter-woven group of numerous hotel concierges. Gustave\\'s message for help, soon finds it\\'s way through concierges M. Ivan (Bill Murray), M. Martin (Bob Balaban), M. Robin (Fisher Stevens), M. Georges (Wallace Wolodarsky), and M. Dino (Waris Ahluwalia).In the end, it is M. Ivan who retrieves Gustave and Zero from the countryside. The concierges have also learned through sources, that Serge has sought refuge in a mountain range known as Gabelmeister\\'s Peak. The concierges have been able to obtain train tickets for the two, as well as Gustave\\'s favorite cologne (though in a much smaller bottle, Ivan regrets to say).With Mr Kovacs now deceased, Dmitri attempts to go over the remnants of his mother\\'s will. It is during this time that the painting \"Boy With Apple\" returns to his mind...and is the first time he has found it missing from the mansion\\'s study! Clotilde the maid then confirms that the painting was removed by Gustave.During this time, Agatha decides to retrieve Boy With Apple\" using Zero\\'s information, but grows wary when she hears footsteps approaching her room.The next day, Serge\\'s sister is found beheaded, the missing body part in a laundry basket (most likely the handiwork of Mr Jopling). Also near her, was a telegram envelope, with its contents missing. Henckel and his soldiers investigate, and also are able to retrieve the telegram\\'s information from the offices, which tell Serge\\'s sister to meet him near Gabelmeister\\'s peak.Gustave and Zero attempt to rendezvous with Serge at an observatory near the summit of Gabelmeister\\'s peak, only for several monks, to direct the two to a monastery high in the hills.In a confessional in the rear of the monastery, Serge informs Gustave and Zero of the death of his sister, as well as his witness to the creation of a second will Madame D had made (in the event she was murdered). Serge explains that Dmitri and his family destroyed it, but that he (Serge) was able to obtain a copy of it. However, before he can tell where it is, Serge falls silent.Gustave and Zero soon find that Serge has been strangled to death, and see Jopling leaving out a side door!The two give chase down the hills, Jopling on skis, and Gustave and Zero on a sled. The wild ride through the snow ends with Jopling pulling off to the side, while the sled plows into a hill, sending Zero into the snow, and Gustave hanging precariously over the edge.Jopling attempts to loosen the icy ledge that the concierge hangs from, when Zero pushes the deranged murderer over the edge, to his death. However, their victory is short-lived, as Henckel is seen across the way, demanding the two not move. Gustave and Zero then take Jopling\\'s motorcycle, amid gunfire from Henckel\\'s troops.Part 5 - The Second Copy of the Second WillThe war finally comes to the residence of The Grand Budapest Hotel, with numerous members of the Military taking over its many rooms. In the absence of Gustave and Zero, concierge duties have now fallen to a man named M. Chuck (Owen Wilson)Retrieving Agatha, both Gustave and Zero have her enter the hotel under the guise of delivering complimentary pastries from Mendl\\'s, as a cover to retrieve \"Boy With Apple\" for them. However, as they watch the front door, Dmitri and his sisters pull up to the entrance!As the family enters the lobby, Dmitri spies Agatha...who quickly turns around and attempts to escape. The two find themselves in an elevator, where Dmitri tears a corner of the paper wrappings...which reveals a portion of the painting to him. Once they arrive on the 6th floor, Dmitri attempts to chase.Meanwhile, fearing for Agatha\\'s life, Gustave and Zero enter the hotel disguised as Mendl\\'s associates...only to encounter Dmitri! Dmitri attempts to kill Gustave, but the gunfire rouses several other Military men on the floors, and an impromptu shootout breaks loose!Henckel soon reaches the floor, demanding all parties cease-firing. However, the silence is broken when Agatha\\'s voice is heard calling for help! Zero sees her dangling off the edge of the third floor suite, the painting hanging nearby. Rushing to her aid, he ends up pitching over the edge along with her, before they both fall off the building...and through the roof of the Mendl\\'s pastry wagon they came in!After \"Boy With Apple\" is retrieved, the Confidential paperwork Serge hid on the back is found. With Gustave, Dmitri, Zero, Agatha, and a number of the hotel staff and armed forces around them, Henckels opens the second will, and reads from its contents. Gustave is not only vindicated of Madame D.\\'s murder, but the second will also gives him numerous portions of numerous businesses she own, including ownership of the Grand Budapest Hotel (of which she was the previously-unknown owner!). In a newspaper article, it is also mentioned that her son Dmitri has disappeared without a trace (and who was suspected of the woman\\'s murder now).As Old Zero continues the story, he notes that Gustave had almost taken on the same aire as the numerous older women he pleasured. As well, with Gustave now having new-found wealth and financial freedom, Zero was then promoted to concierge of the hotel.A scene is shown briefly of Gustave presiding over Zero and Agatha\\'s wedding (with The Society of the Crossed Keys as witnesses), but Zero tells that both his wife and their first child died shortly afterwards, from a fatal disease.The story then switches to Gustave, Zero, and Agatha on a train (some time before her death). During this time, Gustave finally answered the question Zero had asked him some time ago, about if he ever was a lobby boy. The wealthy man says yes, but admits that Zero was a much better lobby boy than he ever was.During the journey, the train stops again in the same barley field as before, and a number of soldiers board checking for papers. While Gustave and Agatha check out, the soldier does not accept Zero\\'s pass. Gustave attempts to use the pass Henckel gave them on their last trip, but the man tears it up, showing that it has no value in their current wartime climate, as the country on the pass (Zubrowkia) has now ceased to exist. Though Gustave threatens the soldier with punishment, the man knocks out Zero with the barrel of his gun, causing the former concierge of the Grand Budapest Hotel, to leap to his feet and assault the man.As Old Zero comes out of his story, the Author inquires what happened to Gustave after that. Old Zero then tells that the soldiers had him killed, and with his death, everything that he owned, was willed to him (Zero).After the story and their meal, the two men head to the front desk, though the concierge is nowhere to be found. Zero then goes behind the counter, and retrieves the keys to both of their rooms. One of the key\\'s large tags reads: \"M Gustave Suite.\"As they wait for the elevator to their rooms, The Author asks Zero if he chose not to sell the hotel to maintain a part of Gustave\\'s world. Zero replies he kept the hotel as a tribute to Agatha, and the best years of his life. He believes the world Gustave had was gone before he worked at the hotel. He departs from the Author. The Author states that he would later travel to South America after visiting the hotel, which he describes as \"marvelous ruins that he never returned to.\"The film closes with a shot of the young woman finishing the Author\\'s book at the cemetery.',\n 'More adventure and hilarity than usual for Anderson \"The Grand Budapest Hotel\" is Wes Anderson\\'s most imaginative effort yet. The perpetually quirky master of symmetry expands his scope in a way not seen since \"The Life Aquatic,\" but this story is infinitely more accessible and entertaining.The film, set in a fictionalized version of Europe, takes an unusual storytelling approach. A now- deceased author (Tom Wilkinson) relays the story behind his most famous novel. When he was younger (played by Jude Law), he visited the once-infamous and decrepit Grand Budapest Hotel, where he comes upon the its owner, Zero Moustafa (F. Murray Abraham), who tells the story of how he came to own it. He\\'s the film\\'s primary narrator, telling the story of how as a young immigrant boy (Tony Revolori) he came under the tutelage of the hotel\\'s flamboyant and unapologetic concierge, M. Gustave (Ralph Fiennes).Fiennes gives a much-welcome change of pace performance as Gustave that is both hilarious and delightful. The tone of \"Grand Budapest\" leans this way as well. Whereas most of Anderson\\'s films move from that quirky, dry humor into stark depictions of relationships, \"Budapest\" almost never loses its comic edge. Far and away, the film earns its label as Anderson\\'s funniest movie. From the outside, the plot looks like a murder mystery farce. Gustave wins the affections of rich old women who stay at the hotel through flattery and sexual services. When one of his most devoted customers, Madame D. (Tilda Swinton) turns up murdered, she wills Gustave the priceless painting \"Boy with Apple,\" provoking the ire of Madame\\'s scornful son, Dmitri, (Adrien Brody). To ensure he gets what\\'s rightfully his, Gustave and Zero steal the painting and Gustave makes a deal to make Zero his heir in exchange for his help. The next day, Gustave is arrested for the murder and Zero tries to help him escape and prove his innocence.Gustave and Zero form a hysterical but meaningful relationship. For all of Gustave\\'s risky, eccentric shenanigans, he grows fond and protective of the boy. This kind of friendship has never been the center of an Anderson film; usually, its sibling or family dysfunction. Consequently, \"Budapest\" is a bit shallower in character depth, opting for moments of pure entertainment than inward exploration. On the plus side, it does make \"Budapest\" more accessible than earlier Anderson films, which could now be considered a trend given that \"Moonrise Kingdom\" was even more so that way.The case of \"Grand Budapest\" extends well beyond Anderson\\'s circle of actors, relegated most of the usual suspects to cameos and small supporting roles. In doing so, Anderson proves he can direct any actor of reasonable talent into fitting with his style. This ensemble is especially vast, though the one real standout is Willem Dafoe as Jopling, Dmitri\\'s brass-knuckled henchman.Anderson really ups the adventure and fun of \"Grand Budapest\" and celebrates his filmmaking style. He adds even more brightness, color, symmetry and hijinks, something he can get away with at this point in his career due to greater audience familiarity. He hasn\\'t gone his deepest, or really pushed himself as an intellectual, but \"Grand Budapest\" proves to be immensely satisfying.~Steven CThanks for reading! Visit moviemusereviews.com for more',\n False]"},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset[100]"]},{"cell_type":"markdown","metadata":{},"source":["### Data Processing\n","\n","The purpose of the following code is to process the raw data so that the processed data can be used for model training and prediction. We will use the `SpacyTokenizer` to split the document into tokens, `ClipSequence` to crop the comments to the specified length, and build a vocabulary based on the word frequency of the training data. Then we attach the [Glove](https://nlp.stanford.edu/pubs/glove.pdf)[2]  pre-trained word vector to the vocabulary and convert each token into the corresponding word index in the vocabulary.\n","Finally get the standardized training data set and verification data set"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Done! Sequence clipping and get length Time=43.28s, #Sentences=900\nDone! Sequence clipping and get length Time=38.23s, #Sentences=100\n"}],"source":["# tokenizer takes as input a string and outputs a list of tokens.\n","tokenizer = nlp.data.SpacyTokenizer('en')\n","length_review = 300\n","length_plot = 600\n","\n","from src.util import mp_tokenizer\n","\n","tokenizer = mp_tokenizer(tokenizer, length_review, length_plot)\n","\n","# Preprocess the dataset\n","train_dataset_token, train_data_lengths = tokenizer.process_dataset(train_dataset)\n","valid_dataset_token, valid_data_lengths = tokenizer.process_dataset(valid_dataset)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":"from src.util import mp_indexer\nembedding = 'glove.42B.300d'\nindexer = mp_indexer(train_dataset_token, embedding)\ntrain_dataset = indexer.process_dataset(train_dataset_token)\nvalid_dataset = indexer.process_dataset(valid_dataset_token)"},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/plain":"900"},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["len(train_dataset)"]},{"cell_type":"markdown","metadata":{},"source":["### Bucketing and DataLoader\n","Since each sentence may have a different length, we need to use `Pad` to fill the sentences in a minibatch to equal lengths so that the data can be quickly tensored on the GPU. At the same time, we need to use `Stack` to stack the category tags of a batch of data. For convenience, we use `Tuple` to combine `Pad` and `Stack`.\n","\n","In order to make the length of the sentence pad in each minibatch as small as possible, we should make the sentences with similar lengths in a batch as much as possible. In light of this, we consider constructing a sampler using `FixedBucketSampler`, which defines how the samples in a dataset will be iterated in a more economic way.\n","\n","Finally, we use `DataLoader` to build a data loader for the training. dataset and validation dataset. The training dataset requires `FixedBucketSampler`, but the validation dataset doesn't require the sampler."]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"FixedBucketSampler:\n  sample_num=900, batch_num=36\n  key=[57, 84, 111, 138, 165, 192, 219, 246, 273, 300]\n  cnt=[7, 9, 8, 13, 6, 5, 14, 2, 1, 835]\n  batch_size=[84, 57, 43, 34, 32, 32, 32, 32, 32, 32]\n"}],"source":["batch_size = 32\n","bucket_num = 10\n","bucket_ratio = 0.5\n","\n","\n","def get_dataloader():\n","    # Construct the DataLoader pad data, stack label and lengths\n","    batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(axis=0), \\\n","                                          nlp.data.batchify.Pad(axis=0),\n","                                          nlp.data.batchify.Stack())\n","\n","    # n this example, we use a FixedBucketSampler,\n","    # which assigns each data sample to a fixed bucket based on its length.\n","    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n","        train_data_lengths,\n","        batch_size=batch_size,\n","        num_buckets=bucket_num,\n","        ratio=bucket_ratio,\n","        shuffle=True)\n","    print(batch_sampler.stats())\n","\n","    # train_dataloader\n","    train_dataloader = gluon.data.DataLoader(\n","        dataset=train_dataset,\n","        batch_sampler=batch_sampler,\n","        batchify_fn=batchify_fn)\n","    # valid_dataloader\n","    valid_dataloader = gluon.data.DataLoader(\n","        dataset=valid_dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        batchify_fn=batchify_fn)\n","    return train_dataloader, valid_dataloader\n","\n","train_dataloader, valid_dataloader = get_dataloader()"]},{"cell_type":"markdown","metadata":{"scrolled":true},"source":["### Weighted softmax cross entropy loss\n","For a regular cross entropy loss, it is calculated by $-\\vec{y}^\\mathsf{T}\\log \\hat{\\vec{y}}$. Since $\\vec{y}$ is actually one-hot, so the value is $-\\log\\hat{y}_i$ where $i$ is the true label's index. To apply weight on this label, we multiply it with some weight value $w_i$, therefore, the loss of class $i$ is $-w_i\\log\\hat{y}_i$."]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["class WeightedSoftmaxCE(nn.HybridBlock):\n","    def __init__(self, sparse_label=True, from_logits=False,  **kwargs):\n","        super(WeightedSoftmaxCE, self).__init__(**kwargs)\n","        with self.name_scope():\n","            self.sparse_label = sparse_label\n","            self.from_logits = from_logits\n","\n","    def hybrid_forward(self, F, pred, label, class_weight, depth=None):\n","        if self.sparse_label:\n","            label = F.reshape(label, shape=(-1, ))\n","            label = F.one_hot(label, depth)\n","        if not self.from_logits:\n","            pred = F.log_softmax(pred, -1)\n","\n","        weight_label = F.broadcast_mul(label, class_weight)\n","        loss = -F.sum(pred * weight_label, axis=-1)\n","        \n","        return loss"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":"class AlignAttention(nn.Block):\n    def __init__(self, **kwargs):\n        super(AlignAttention, self).__init__(**kwargs)\n    \n    def forward(self, inp_left, inp_right):\n        # input dimension is of (batch_size, seq_len, embed_size)\n        # att dimension is of (batch_size, seq_len_left, seq_len_right)\n        att = nd.batch_dot(inp_left, nd.transpose(inp_right, axes = (0, 2, 1)))\n        # inp_left_dot dimention is of (batch_size, seq_left, embed_size)\n        inp_left_dot = nd.batch_dot(nd.softmax(att, axis=-1), inp_right)\n        # inp_right_dot dimension is of (batch_size, seq_right, embed_size)\n        inp_right_dot = nd.batch_dot(nd.softmax(nd.transpose(att, axes=(0, 2, 1)), axis=-1), inp_left)\n        # concat original (lstm output, dot multiplier, substraction, elementwise product)\n        # therefore, the real size is (batch_size, seq_len_left/right, embed_size*4)\n        aug_left = nd.concat(inp_left, inp_left_dot, inp_left-inp_left_dot, inp_left*inp_left_dot, dim=-1)\n        aug_right = nd.concat(inp_right, inp_right_dot, inp_right-inp_right_dot, inp_right*inp_right_dot, dim=-1)\n        return aug_left, aug_right, att"},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["class EnhancedSeqInfer(nn.Block):\n","    def __init__(self, vocab_len, embsize, nhidden, nlayers, nfc, nclass, drop_prob, **kwargs):\n","        super(EnhancedSeqInfer, self).__init__(**kwargs)\n","        with self.name_scope():\n","            # dropout prob\n","            self.drop_prob = drop_prob\n","            # word embedding\n","            self.embedding_layer = nn.Embedding(vocab_len, embsize)\n","            # first lstm, from sentence embed to hidden outputs\n","            self.bilstm1 = rnn.LSTM(nhidden, num_layers=nlayers, dropout=drop_prob, bidirectional=True)\n","            # second lstm, from augmented embed to m\n","            self.bilstm2 = rnn.LSTM(nhidden, num_layers=1, dropout=drop_prob, bidirectional=True)\n","            # enhancement\n","            self.align_att = AlignAttention()\n","            # this layer is used to output the final class\n","            self.output_layer = nn.HybridSequential()\n","            self.output_layer.add(nn.Dense(nfc, activation='tanh'), nn.Dropout(rate=drop_prob), \\\n","                                  nn.Dense(nfc, activation='tanh'), nn.Dropout(rate=drop_prob), \\\n","                                  nn.Dense(nclass))\n","\n","    def forward(self, inp_left, inp_right):\n","        # inp is a list containing left_text and right_text\n","        # their size: [batch, token_idx]\n","        # inp_embed_left/right size: [batch, seq_len, embed_size]\n","        inp_embed_left = self.embedding_layer(inp_left)\n","        inp_embed_right = self.embedding_layer(inp_right)\n","        # rnn requires the first dimension to be the time steps, output is (seq_len, batch_size, embed_size)\n","        h_output_left = self.bilstm1(nd.transpose(inp_embed_left, axes=(1, 0, 2)))\n","        h_output_right = self.bilstm1(nd.transpose(inp_embed_right, axes=(1, 0, 2)))\n","        m_left, m_right, att = self.align_att(nd.transpose(h_output_left, axes=(1, 0, 2)), \\\n","                                                      nd.transpose(h_output_right, axes=(1, 0, 2)))\n","        # apply another layer of lstm\n","        # v_left/right shape is (seq_len, batch_size, embed_size)\n","        v_left = self.bilstm2(nd.transpose(m_left, axes=(1, 0, 2)))\n","        v_right = self.bilstm2(nd.transpose(m_right, axes=(1, 0, 2)))\n","        # restore v's shape (batch_size, seq_len, embed_size)\n","        v_left = nd.transpose(v_left, axes=(1, 0, 2))\n","        v_right = nd.transpose(v_right, axes=(1, 0, 2))\n","        # apply max pooling 1D and avg pooling 1D\n","        v_left_avg = nd.sum(v_left, axis=1) / v_left.shape[1]\n","        v_right_avg = nd.sum(v_right, axis=1) / v_right.shape[1]\n","        v_left_max = nd.max(v_left, axis=1)\n","        v_right_max = nd.max(v_right, axis=1)\n","        # concatenate these 4 matrices\n","        dense_input = nd.concat(v_left_avg, v_left_max, v_right_avg, v_left_max, dim=-1)\n","        \n","        output = self.output_layer(dense_input)\n","        return output, att"]},{"cell_type":"markdown","metadata":{},"source":["### Configure parameters and build models\n","`test_model` is set to `True` if we want to just load parameters and test on the model. Otherwise, it is set to `False` during training."]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"EnhancedSeqInfer(\n  (embedding_layer): Embedding(27841 -> 300, float32)\n  (bilstm1): LSTM(None -> 300, TNC, num_layers=2, dropout=0.5, bidirectional)\n  (bilstm2): LSTM(None -> 300, TNC, dropout=0.5, bidirectional)\n  (align_att): AlignAttention(\n  \n  )\n  (output_layer): HybridSequential(\n    (0): Dense(None -> 256, Activation(tanh))\n    (1): Dropout(p = 0.5, axes=())\n    (2): Dense(None -> 256, Activation(tanh))\n    (3): Dropout(p = 0.5, axes=())\n    (4): Dense(None -> 2, linear)\n  )\n)\n"}],"source":["test_model = False\n","vocab_len = len(indexer.vocab)\n","emsize = 300    # word embedding size\n","nhidden = 300   # lstm hidden_dim\n","nlayers = 2     # lstm layers\n","\n","# final fc layer's number of hidden units and predicted number of classes\n","nfc = 256\n","nclass = 2\n","\n","drop_prob = 0.5\n","\n","ctx = mx.gpu(0)\n","\n","model = EnhancedSeqInfer(vocab_len, emsize, nhidden, nlayers, nfc, nclass, drop_prob)\n","\n","if test_model:\n","    model.load_parameters('model/esim-0.79.params', ctx=ctx)\n","else:\n","    model.initialize(init=init.Xavier(), ctx=ctx)\n","# Attach a pre-trained glove word vector to the embedding layer\n","model.embedding_layer.weight.set_data(indexer.vocab.embedding.idx_to_vec)\n","# fixed the embedding layer\n","model.embedding_layer.collect_params().setattr('grad_req', 'null')\n","\n","print(model)\n","\n","train_curve, valid_curve = [], []"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["def calculate_loss(x_left, x_right, y, model, loss, class_weight):\n","    pred, att = model(x_left, x_right)\n","    y = nd.array(y.astype('int32', copy=False), ctx=ctx)\n","    if loss_name == 'sce':\n","        l = loss(pred, y)\n","    elif loss_name == 'wsce':\n","        l = loss(pred, y, class_weight, class_weight.shape[0])\n","    else:\n","        raise NotImplemented\n","    return pred, l"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["def one_epoch(data_iter, model, loss, trainer, ctx, is_train, epoch, class_weight=None, loss_name='sce'):\n","\n","    loss_val = 0.\n","    total_pred = []\n","    total_true = []\n","    n_batch = 0\n","\n","    for batch_x_left, batch_x_right, batch_y in data_iter:\n","        batch_x_left = batch_x_left.as_in_context(ctx)\n","        batch_x_right = batch_x_right.as_in_context(ctx)\n","        batch_y = batch_y.as_in_context(ctx)\n","\n","        if is_train:\n","            with autograd.record():\n","                batch_pred, l = calculate_loss(batch_x_left, batch_x_right, \\\n","                                               batch_y, model, loss, class_weight)\n","\n","            # backward calculate\n","            l.backward()\n","\n","            # update parmas\n","            trainer.step(batch_x_left.shape[0])\n","\n","        else:\n","            batch_pred, l = calculate_loss(batch_x_left, batch_x_right, \\\n","                                           batch_y, model, loss, class_weight)\n","\n","        # keep result for metric\n","        batch_pred = nd.argmax(nd.softmax(batch_pred, axis=1), axis=1).asnumpy()\n","        batch_true = np.reshape(batch_y.asnumpy(), (-1, ))\n","        total_pred.extend(batch_pred.tolist())\n","        total_true.extend(batch_true.tolist())\n","        \n","        batch_loss = l.mean().asscalar()\n","\n","        n_batch += 1\n","        loss_val += batch_loss\n","\n","        # check the result of traing phase\n","        if is_train and n_batch % 400 == 0:\n","            print('epoch %d, batch %d, batch_train_loss %.4f, batch_train_acc %.3f' %\n","                  (epoch, n_batch, batch_loss, accuracy_score(batch_true, batch_pred)))\n","\n","    # metric\n","    F1 = f1_score(np.array(total_true), np.array(total_pred), average='binary')\n","    acc = accuracy_score(np.array(total_true), np.array(total_pred))\n","    loss_val /= n_batch\n","\n","    if is_train:\n","        print('epoch %d, learning_rate %.5f \\n\\t train_loss %.4f, acc_train %.3f, F1_train %.3f, ' %\n","              (epoch, trainer.learning_rate, loss_val, acc, F1))\n","        train_curve.append((acc, F1))\n","        # declay lr\n","        if epoch % 2 == 0:\n","            trainer.set_learning_rate(trainer.learning_rate * 0.9)\n","    else:\n","        print('\\t valid_loss %.4f, acc_valid %.3f, F1_valid %.3f, ' % (loss_val, acc, F1))\n","        valid_curve.append((acc, F1))"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["def train_valid(data_iter_train, data_iter_valid, model, loss, trainer, \\\n","                ctx, nepochs, class_weight=None, loss_name='sce'):\n","\n","    for epoch in range(1, nepochs+1):\n","        start = time.time()\n","        # train\n","        is_train = True\n","        one_epoch(data_iter_train, model, loss, trainer, ctx, is_train, epoch, class_weight, loss_name)\n","\n","        # valid\n","        is_train = False\n","        one_epoch(data_iter_valid, model, loss, trainer, ctx, is_train, epoch, class_weight, loss_name)\n","        end = time.time()\n","        print('time %.2f sec' % (end-start))\n","        print(\"*\"*100)"]},{"cell_type":"markdown","metadata":{},"source":["### Train"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":"class_weight = None\nloss_name = 'sce'\noptim_name = 'adam'\nlr = 0.001\nclip = 2.5\nnepochs = 10\n\ntrainer = gluon.Trainer(model.collect_params(), optim_name, {'learning_rate': lr, 'clip_gradient': clip})\n\nif loss_name == 'sce':\n    loss = gluon.loss.SoftmaxCrossEntropyLoss()\nelif loss_name == 'wsce':\n    loss = WeightedSoftmaxCE()\n    # the value of class_weight is obtained by counting data in advance. It can be seen as a hyperparameter.\n    class_weight = nd.array([1., 3.], ctx=ctx)\nelse:\n    print('loss function {} is not implemented!'.format(loss_name))\n    raise NotImplemented"},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"epoch 1, learning_rate 0.00100 \n\t train_loss 0.6014, acc_train 0.716, F1_train 0.200,\n\t valid_loss 0.6176, acc_valid 0.680, F1_valid 0.059,\ntime 26.95 sec\n****************************************************************************************************\nepoch 2, learning_rate 0.00100 \n\t train_loss 0.5949, acc_train 0.718, F1_train 0.201,\n\t valid_loss 0.5887, acc_valid 0.660, F1_valid 0.320,\ntime 26.78 sec\n****************************************************************************************************\n"}],"source":["# train and valid\n","train_valid(train_dataloader, valid_dataloader, model, loss, \\\n","            trainer, ctx, nepochs, class_weight=class_weight, loss_name=loss_name)"]},{"cell_type":"markdown","metadata":{},"source":["### Save the model and the training, validation curves"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-1-435e70efc0e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model/esim-{:.4}.params'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_curve\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'acc_record'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_curve\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_curve\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":"model.save_parameters('model/esim-{:.4}.params'.format(str(valid_curve[-1][0])))\nwith open('acc_record', 'w') as f:\n    f.write(str(train_curve)+'\\n')\n    f.write(str(valid_curve))"},{"cell_type":"markdown","metadata":{},"source":["### Visualize the training and validation curves\n","We can find in the plot that after how many epochs the model performs best on validation set without overfitting."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"from ast import literal_eval\nwith open('acc_record', 'r') as f:\n    train_curve, valid_curve = [ literal_eval(line) for line in f]"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"train_acc = [ acc for acc, _ in train_curve ]\ntrain_f1 = [ f1 for _, f1 in train_curve ]\nvalid_acc = [ acc for acc, _ in valid_curve ]\nvalid_f1 = [ f1 for _, f1 in valid_curve ]\nepochs = [ i for i in range(1, len(valid_curve)+1) ]\nimport matplotlib.pyplot as plt\nplt.title('Accuracy')\nplt.plot(epochs, train_acc, label='train', color='orange')\nplt.plot(epochs, valid_acc, label='validation', color='green')\nplt.legend(loc='upper left')\nplt.show()\nplt.title('F1')\nplt.plot(epochs, train_f1, label='train', color='orange')\nplt.plot(epochs, valid_f1, label='validation', color='green')\nplt.legend(loc='upper left')\nplt.show()"},{"cell_type":"markdown","metadata":{},"source":["### Test on randomly made up plot and review by myself\n","Now we will randomly input a movie plot summary and its review to see if the model decide the review is a spoiler or not."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"right = 'Tim killed Tom on a winter evening, and Sarah saw this scene.'\nleft = 'Sarah saw Tom killed, it is Tim to blame'\nleft_token = indexer.vocab[tokenizer.tokenizer(left)]\nright_token = indexer.vocab[tokenizer.tokenizer(right)]"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"left_input = nd.array(left_token, ctx=ctx).reshape(1,-1)\nright_input = nd.array(right_token, ctx=ctx).reshape(1,-1)\npred, att = model(left_input, right_input)\npred, nd.argmax(pred, axis=1).asscalar()"},{"cell_type":"markdown","metadata":{},"source":["### Visualize the soft alignment"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false},"outputs":[],"source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nvis_att = np.squeeze(att.asnumpy())\nplt.figure(figsize=vis_att.T.shape)\nsns.heatmap(vis_att, xticklabels=tokenizer.tokenizer(right), yticklabels=tokenizer.tokenizer(left), center=0.)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}