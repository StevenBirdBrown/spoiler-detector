{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Enhanced Sequence Inference Model\n","The model is based on [this paper](https://arxiv.org/abs/1609.06038).\n","\n","### Import Related Packages"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":"import os\nimport json\nimport zipfile\nimport time\nimport itertools\n\nimport numpy as np\nimport mxnet as mx\nimport multiprocessing as mp\nimport gluonnlp as nlp\n\nfrom mxnet import gluon, nd, init\nfrom mxnet.gluon import nn, rnn\nfrom mxnet import autograd, gluon, nd\nfrom d2l import try_gpu\nimport pandas as pd\n\n# sklearn's metric function to evaluate the results of the experiment\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# fixed random number seed\nnp.random.seed(9102)\nmx.random.seed(9102)"},{"cell_type":"markdown","metadata":{},"source":["## Data pipeline\n","\n","### Load Dataset\n","\n","See [dataloader](data_loader.ipynb) for how the training samples are obtained."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":"(5000, 4)"},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":"data_folder = 'data/imdb/'\nfile_name = 'train.csv'\nfile_path = data_folder + file_name\n# dev: True - only use a small dataset\n# create_vocab: True - create a new vocabulary from training data\ndev = True\n# load train file\nif dev:\n    # load only n rows\n    nrows = 1000\n    data = pd.read_csv(file_path, nrows=nrows)\nelse:\n    # load as many as possible\n    data = pd.read_csv(file_path)\ndata.shape"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":"((5000, 4),     movie_id                                          sentence1  \\\n 0  tt0097576  The story opens in Monument Valley, Utah, in 1...   \n 1  tt2488496  Luke Skywalker has vanished. In his absence, t...   \n 2  tt0407887  In voiceover, Irish-American mobster Frank Cos...   \n 3  tt0209144  This is a complex story about Leonard Shelby (...   \n 4  tt1285016  In October 2003, Harvard University student Ma...   \n \n                                            sentence2  label  \n 0  Chemistry between Ford and Connery puts this f...  False  \n 1  You did it, J.J. ... You have ruined Star Wars...   True  \n 2  No respect! This remake of \"Internal Affairs\",...  False  \n 3  Scientific and psychological thriller. Who kne...   True  \n 4  A Grossly Overrated But Still Very Good Film A...  False  )"},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":"data.dropna(inplace=True)\ndata.shape, data.head()"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"label2num = {'contradiction':0, 'neutral':1, 'entailment':2}"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":"(4750, 250)"},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":"# create a list of review a label paris.\ndataset = [[left, right, label] for left, right, label in \\\n           zip(data['sentence1'], data['sentence2'], data['label']) if label!='-' ]\n# randomly divide one percent from the training set as a verification set.\ntrain_dataset, valid_dataset = nlp.data.train_valid_split(dataset, valid_ratio=.1\nlen(train_dataset), len(valid_dataset)"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":"iling. This means that Lincoln Six Echo himself would \\'be chosen to go to the Island\\' by that time, for organ harvest.Lincoln tells his sponsor that he and Jordan have come to him, hoping to expose the truth of the Institute to the public. Tom Lincoln says that he can\\'t believe it himself, even though he\\'s standing face to face with his own insurance policy. But Lincoln Six Echo happens to turn and sees a man on the television screen that he recognizes from the Agnate population at the Institute-- the very President of the United States has a clone at the Institute. Lincoln Six Echo understands that this man is giving a broadcast announcement to large groups of people. If he and Tom Lincoln can get onto the news together, the Institute will be exposed. Jordan tells Tom Lincoln that thousands of people, everyone she and Lincoln Six Echo know at the Institute, are going to die there. When Jordan takes Lincoln Six Echo\\'s hand, Tom Lincoln finally smiles at her and says he just needs to get his shoes, and the\\'ll bring them to the local NBC news station.But as Tom Lincoln goes upstairs, he quietly places a call to the Merrick Institute and demands the Biotech receptionist find someone who can explain to him why his insurance policy is in his house.As Tom Lincoln comes back downstairs, Jordan turns to Lincoln Six Echo, whispering to him with suspicion. She\\'s read Tom Lincoln\\'s eyes-= just like Lincoln Six Echo, Tom Lincoln\\'s eyes didn\\'t smile along with his mouth. She knows he lied to her. Lincoln Six Echo says that Jordan will stay behind in case nobody believes them.The two Lincolns get into one of Tom\\'s fastest, fanciest cars. Lincoln Six Echo drives, much to Tom Lincoln\\'s surprise. The half-million dollar Cadillac is activated by a thumbprint, and Lincoln Six Echo\\'s is a perfect match. Tom Lincoln tells him that he cost five million dollars to have produced, which, as Tom Lincoln says, is a small price to cheat death.In the house, Jordan goes upstairs to Tom Lincoln\\'s room and finds an open briefcase used to house two pistols. One of them is missing. She picks up the smaller one, weighing it in her hand.Lincoln must stop at a red light, and suddenly the light itself falls down just in front of the car-- Laurent\\'s men have caught up with them in a Blackhawk helicopter. At that moment, Tom pulls his own pistol and tells Lincoln Six Echo that he\\'s not ready to die for him. Saying that the feeling is mutual, Lincoln floors the gas pedal again, racing forward, throwing Tom off balance and leaving him unable to shoot, lest the car go careening out of control and crashing. When Tom Lincoln tries to wrestle for control, Lincoln Six Echo bites his hand and elbows him in the face.Laurent and his men follow in hot pursuit as Lincoln races down the highway, struggling to maintain control of the car while also fighting off Tom. Laurent\\'s men corner the two in the interior of an abandoned museum. But as the two Lincolns emerge from the car, fighting for control of Tom Lincoln\\'s gun, Laurent and his men freeze. Tom Lincoln\\'s glasses have fallen off in the struggle, and there\\'s no way to tell him apart from Lincoln Six Echo. The two Lincolns argue futilely back and forth, each claiming to be the real Tom Lincoln.But Lincoln Six Echo has a trump card. He realizes his Agnate bracelet, removed from his wrist by Tom, is in his pocket. In the confusion, he slips the bracelet out of his pocket and snaps it around Tom Lincoln\\'s wrist. Laurent, suddenly spotting the bracelet, shoots Tom Lincoln to death. As Lincoln and Laurent leave, Laurent asks Lincoln where Jordan is. Lincoln says that she went out of state, to look for Sarah Jordan.As Laurent points out to Lincoln that he\\'s been witness to certain \\'trade secrets,\\' Lincoln\\'s answer subtly indicates that he\\'s the clone. Laurent\\'s tone hints that he\\'s picked up on this, but is playing along. Lincoln says that people will do anything to survive, and he just wants to live, without regard for how.Laurent reports in to Dr. Merrick, who takes the news with extreme relief and happiness. Meanwhile, Lincoln returns to the house, and to Jordan. Jordan is initially suspicious, coming up from behind and pointing her gun at Lincoln. But as Lincoln turns around and says it\\'s him, she sees the truth in his eyes. Jordan goes to Lincoln and starts kissing him, and the two make love. Jordan telle Lincoln that the Island is real after all: It\\'s they, themselves. It\\'s the spirit inside them.Dr. Merrick and the Incubation Chief are in the conditoining room for newly grown Agnates undergoing conditioning before integration into the Institute Agnate population. The entire batch of Echo, Foxtrot, Gamma, and Helo generation Agnates have shown similar neural pattern scans; inheriting memory imprints from their sponsors. The entire Agnate program has been predicated on predictability. But Lincoln Six Echo has shown that the universal trait of human curiosity has undermined it. The Institute is faced with a very large number of Agnates that have the trait for defiant behavior and the ability and nature to question Agnate life at the Institute. Dr. Merrick believes the only solution is a recall: the disposal of over 200 million dollars worth of product... the termination of almost half the Agnate population.Charlie Whitman calls Lincoln at his house to inform him of the recall. Lincoln, posing as his sponsor, Tom Lincoln, listens in horror as Whitman tells him that four generations of clones are going to be destroyed and re-grown. Whitman asks if Merrick reps can pick him up that afternoon to re-scan him so that his policy can be re-grown.Lincoln and Jordan know that they have to get away. Lincoln, believed to be the real Tom Lincoln, now has control of his sponsor\\'s vast fortune. Once they finish building a full-scale Renovatio, they can ride south and find a new life together in anonymity. But first, they have to do something about the recall. All the other clones, who, to them, are very much real people, have to be saved. And the mechanism for the recall, as Dr. Merrick explains to wealthy investors, is already in place-- large numbers of Agnates will all be chosen simultaneously as \"Lottery winners\" to \"go to the Island.\" One such Lottery is shown, and Jones Three Echo is one of the ones chosen.Dr. Merrick requests a board vote on the termination of the infected products. The vote is unanimous in favor. Technicians and medical personnel are seen going to the Incubation room and prematurely extracting incubating Agnates from the incubation pods. Newly extracted Agnates going pre-integration conditioning are seen being administered lethal injections.Lincoln has a plan to save the living Agnates: He remembers the Holographic Control room. If he can shut it down, the Agnates at the Institute will all see the truth about the outside world; the holographic grids throughout the Institute hide it from them.Lincoln and Jordan put their plan into action. Lincoln studies all the photographs of Tom Lincoln and dresses himself up accordingly in Tom\\'s clothes. He goes on the Merrick helicopter to be re-scanned. Jordan obtains a Ben and Jerry\\'s Ice-cream stand and serves ice cream to young children. She then swipes McCord\\'s credit card across the reader. Laurent\\'s men pick up the scan immediately and mobilize. Jordan sits on a playground swing, calmly swinging on it. Laurent and his men converge, seizing her. Jordan properly plays along, acting frightened and crying as they bring her back to the Institute. Laurent looks thoughtfully at the thermal implant under the skin of Jordan\\'s wrist as she anxiously runs her fingers across it.Whitman meets Lincoln in the client waiting room, and brings him to where he\\'ll be scanned. Lincoln again plays along, acting indifferent and impatient to get the whole thing done and over with. Meanwhile, Jordan is brought to Surgery prep for organ harvest. Her kidneys, lungs, and heart are scheduled for transplant into her sponsor.In the data storage room, as Whitman turns his back and starts making idle small talk again, Lincoln suddenly clubs him unconscious with the butt of his gun. Meanwhile, as Jordan is on the gurney in the surgery prep room, she quietly eases her own gun out of her pants waistband-- Laurent and his men never having bothered to search her. She shoots the security agent in the leg, taking his walkie and the surgeon\\'s tri-key, and dons a medical coat. Locking them in the surgery room, she uses hand-drawn sketches Lincoln made of the medical sector\\'s layout, and traces her way to the entrance point, opening the door for Lincoln.As the two of them continue through the sector, they happen to pass by where a large group of Agnates are waiting for what they think is transport to the Island. Jordan gives Lincoln her tri-key and tells him he has to go on, to the holographic control grid. Lincoln goes into Sector Six while Jordan discreetly begins following the security escorts preparing to dispose of the Agnates.Laurent meets Dr. Merrick in his office. Merrick expresses his gratitude for Laurent\\'s services and has Whitman write a check to pay him. Laurent asks about Sarah Jordan, and Dr. Merrick dismissively says that her prognosis isn\\'t good-- even with the organ transplants, she\\'s suffered enough brain damage that she probably won\\'t recover. Despite this, the transplant is going through; the privilege she\\'s paid for.Laurent tells Dr. Merrick that his father was part of a political rebellion in his native Burkina Faso. When the rebellion was crushed and Laurent\\'s father slain, he and all his brothers were branded on their palms to show that, as children of a rebel, they were less than human. With all he\\'s seen and done, he\\'s learned that to many, war is a business. He asks Dr. Merrick when killing became a business for him.Dr. Merrick explains that to him, it\\'s not about the killing. He doesn\\'t see the Agnates as more than tools; medical instruments. He doesn\\'t see them as having souls. Dr. Merrick gives life to stricken people, and in two years, predicts he\\'ll be able to cure children\\'s leukemia-- how many people can say that? Laurent quietly answers that only Dr. Merrick, and God, will be able to... and that this is exactly the way Dr. Merrick wants it.As Lincoln continues on his way to the power grid, Merrick biotech morgue workers are examining the body of Tom Lincoln to verify the cause of death for their records. The coroner suddenly notices that the Agnate bracelet is on the wrong wrist-- the right wrist (the hand Tom Lincoln held his gun in), while Agnates have their bracelets on their left wrist. The coroner immediately calls Dr. Merrick, who is horrified at the news that the real Tom Lincoln is the one who was killed.A full security alert is issued for Lincoln Six Echo, and Laurent hears the blaring of alarms. Lincoln has made it to the control room of the holographic power grid. When he shoots at the locked doors, security picks up the disturbance and notifies Dr. Merrick. Meanwhile, Jordan is following the \\'chosen\\' Agnates and their security escorts. As they go down a lift, Jordan turns and sees Laurent behind her. She aims her gun at him, but can\\'t shoot. Laurent slowly approaches and deftly disarms her.The \\'chosen\\' Agnates are loaded into what looks roughly like a lift of some sort, but Jones Three Echo suspects something is wrong-- and he\\'s right; he and the other Agnates have been placed into an incineration chamber. But as the security agent begins to activate it, Jordan and Laurent are there. Laurent is now working with Jordan to save all the Agnates at the Institute. They force the security personnel to shut down the incinerator and open it, releasing the Agnates inside.Lincoln has made his way to the reactor that controls the holographic power grid, and sees how to shut it down. As he pulls the lever to overload the reactor fan, he\\'s attacked by Dr. Merrick. Merrick is amazed and impressed-- Lincoln Six Echo could have taken over his sponsor\\'s life, but instead has come back. When he says, \\'You truly are unique, Six Echo,\\' Lincoln asserts, \\'My name is Lincoln!\\' and he fights back. Lincoln and Merrick grapple and struggle through the holographic reactor chamber. The reactor fan overloads and the reactor begins surging out of control. On a walkway in the control room, Dr. Merrick has control of Lincoln via a security issued harpoon/stun gun. Setting the gun down, he chooses to strangle Lincoln with his bare hands. Seeing the gun on the floor, Lincoln grabs it and fires the harpoon right through Merrick\\'s throat, wrapping the cable around his neck. The turbine fans break loose from the reactor and spin wildly about the control room, destroying anything they collid with. The walkway on which Merrick and Lincoln struggle is broken apart, and Dr. Merrick is hanged by the harpoon cable. His body acting as a counterweight, Lincoln is able to climb to safety.The holographic grid is destroyed, All throughout the Agnate quarters, the Agnates see the holograms fade. They rush in frenzies, congregating around rays of natural sunlight shining through windows in the Institute buildings. The Agnates all begin to find their way out of the Institute and out of the bunker in which it\\'s located, emerging into the outside world. Lincoln and Jordan find each other and begin kissing. They see Laurent behind them, and they and he look at each other with a mutual expression of gratitude.The Island closes with Jordan and Lincoln aboard a completed Renovatio as it cruises through crystal blue waters, taking them to a new life.',\n 'Not The Usual Michael Bay Nonsense Michael Bay\\'s science-fiction thriller \"The Island\" (*** out of ****) provides a refreshing change of pace from a summer glutted with prequels, sequels, remakes, and comic book superhero sagas. Despite its abysmal weekend box office figures and the usual critical carping that accompany all his movies, \"The Island\" qualifies as Bay\\'s best movie since \"The Rock\" (1996), that far-fetched but supercharged nonsense about Nicolas Cage & Sean Connery escaping from Alcatraz. A number of things set \"The Island\" apart from its formula-driven competition. First, superstars don\\'t flesh out the cast. Sure, Ewan McGregor receives top billing, but nobody would remember him as Obi-Wan Kenobi in the latest \"Star Wars\" trilogy, because the British actor wields neither a light saber nor does he wear a beard and a bathrobe. Meanwhile, Scarlett Johansson hasn\\'t acquired the clout that Drew Barrymore and Jennifer Lopez command. Nevertheless, she definitely looks bound for stardom in her own right. As a couple, McGregor and Johansson don\\'t exactly set the world aflame like Angelina Jolie & Brad Pitt did in \"Mr. & Mrs. Smith.\" Second, our heroes lack career professional skills, trades, or crafts. Meaning, they aren\\'t cops, nurses, pilots, criminals, attorneys, or scientists out to save the world. Instead, they resemble grown-up children without street smarts confronting a hostile society that deliberately misleads them every step of the way. Third, the writers borrow important plot ideas from other sci-fi epics like \"Logan\\'s Run,\" \"Soylent Green,\" \"THX-1138,\" \"Coma,\" \"Westworld\" and \"Planet of the Apes,\" so that \"The Island\" emerges unmistakably as an homage to those late 1960s/early 1970s Watergate-inspired paranoid sci-fi white-knucklers that you enjoy with greater satisfaction based on less information about their plots. Don\\'t discuss \"The Island\" with anybody unless you want them to blow its secrets and surprises. Fourth, Michael Bay has actually helmed a feature-length film that has something relevant to say about our society. Of course, this doesn\\'t discourage the \"Armageddon\" director from hyping up the third act with trigger-happy police, explosions, auto chases, improbable surprises, and the usual demolition derby tactics that Bay has a knack for staging in big, overripe blockbuster blow-outs like \"Pearl Harbor\" and \"Bad Boys\" movies.Cast as Lincoln Six Echo, Ewan McGregor of \"Trainspotting\" plays a compliant but curious nobody in a futuristic society where the inhabitants display no curiosity about their state of affairs and crave no desire to defy authority. Lincoln\\x97we learn--survived a major catastrophe along with several thousand other people that has left the Earth a contaminated wasteland. As a result, those like Lincoln Echo Six must live in quarantine in a Utopian, man-made research facility. The staff monitors their patients\\' moods, their diet, and their overall behavior. Impatiently, Lincoln and the others bide their time for the opportunity to win a daily lottery and pick up the prize: a trip to a lush paradise known as The Island, billed as \"nature\\'s last remaining pathogen-free zone.\" Lincoln stands apart from his friends and fellows, because he allows his curiosity to get him into big trouble. Everything in this huge, antiseptic research facility may not be all it seems, and Lincoln begins to believe the worst. One of Lincoln\\'s unhappy fellow workers insists that the lottery is fixed. Meanwhile, Lincoln gets friendly with Jordan Two Delta (pretty Scarlett Johansson of \"Lost in Translation\") and the two become quite chummy. In the sealed off research facility, nobody may engage in sexual behavior, and the vigilant staff maintain a \\'proximity\\' rule that prevents a man and a woman from embracing. Lincoln Six Echo stumbles onto the truth when a cynical research facility technician, McCord (Steve Buscemi of \"Con-Air\"), warns our naïve hero about trusting humans. Eventually, Lincoln figures out a way to break out and Jordan accompanies him. A desperate Merrick (Sean Bean of \"GoldenEye\") hires ex-military, South African mercenary, Laurent (Djimon Hounsou of \"Gladiator\") to recapture Lincoln and Jordan before they can get to the world beyond their clean-scrubbed confines.The first third of \"The Island\" poses an intriguing premise that Hollywood hasn\\'t handled as forcefully since Ridley Scott\\'s \"Blade Runner.\" This politically incorrect actioneer about a notorious controversy and its possible repercussions on the life insurance industry seems almost inevitable. Typically, this kind of delicate but contentious subject matter fares better in the realm of science fiction, and the story occurs15 years into the future. About a half hour into this hokum, however, you should be \\'wise\\' to what is really happening in the closely-guarded world of Lincoln and Jordan. Mind you, \"The Island\" is predictable from start-to-finish. Most of you will be one jump ahead of the protagonists. Literary critics call this audience empowerment \\'dramatic irony,\\' because we know more than the heroes. Nevertheless, both Lincoln and Jordan stack up as very likable and sympathetic characters. You\\'ll want them to triumph over clearly impossible odds. Incredibly, the able supporting cast and characters prove just as congenial, meaning you\\'ll care about them, too. The second third of \"The Island\" imitates a horror movie without excessive blood and gore. Like all provocative sci-fi thrillers, \"The Island\" delivers a vision of the future that is as chilling as anything in a holocaust movie. Despite the dreadful subject matter, Bay handles it in a way that makes his points without sickening us. The final third is your average knuckle-busting chase movie with melodramatic story reversals that undoubtedly will aggravate nit-pickers who demand realism, even when realism robs whatever charm any movie contains. For example, our clueless heroes extract themselves from one cliffhanger situation after another that people with their limited brainpower couldn\\'t pull off considering the savvy and resources at the disposal of their assailants! You\\'ll have to remind yourself repeatedly that \"The Island\" is only a movie, but at least it isn\\'t a prequel, sequel, remake, or superhero saga!',\n True]"},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":"train_dataset[100]"},{"cell_type":"markdown","metadata":{},"source":["### Data Processing\n","\n","The purpose of the following code is to process the raw data so that the processed data can be used for model training and prediction. We will use the `SpacyTokenizer` to split the document into tokens, `ClipSequence` to crop the comments to the specified length, and build a vocabulary based on the word frequency of the training data. Then we attach the [Glove](https://nlp.stanford.edu/pubs/glove.pdf)[2]  pre-trained word vector to the vocabulary and convert each token into the corresponding word index in the vocabulary.\n","Finally get the standardized training data set and verification data set"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Done! Sequence clipping and get length Time=68.09s, #Sentences=4750\nDone! Sequence clipping and get length Time=45.03s, #Sentences=250\n"}],"source":"# tokenizer takes as input a string and outputs a list of tokens.\ntokenizer = nlp.data.SpacyTokenizer('en')\nlength_review = 300\nlength_plot = 600\n\nfrom src.util import mp_tokenizer\n\ntokenizer = mp_tokenizer(tokenizer, length_review, length_plot)\n\n# Preprocess the dataset\ntrain_dataset_token, train_data_lengths = tokenizer.process_dataset(train_dataset)\nvalid_dataset_token, valid_data_lengths = tokenizer.process_dataset(valid_dataset)"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"from src.util import mp_indexer\nembedding = 'glove.42B.300d'\nindexer = mp_indexer(train_dataset_token, embedding)\ntrain_dataset = indexer.process_dataset(train_dataset_token)\nvalid_dataset = indexer.process_dataset(valid_dataset_token)"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":"4750"},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":"len(train_dataset)"},{"cell_type":"markdown","metadata":{},"source":["### Bucketing and DataLoader\n","Since each sentence may have a different length, we need to use `Pad` to fill the sentences in a minibatch to equal lengths so that the data can be quickly tensored on the GPU. At the same time, we need to use `Stack` to stack the category tags of a batch of data. For convenience, we use `Tuple` to combine `Pad` and `Stack`.\n","\n","In order to make the length of the sentence pad in each minibatch as small as possible, we should make the sentences with similar lengths in a batch as much as possible. In light of this, we consider constructing a sampler using `FixedBucketSampler`, which defines how the samples in a dataset will be iterated in a more economic way.\n","\n","Finally, we use `DataLoader` to build a data loader for the training. dataset and validation dataset. The training dataset requires `FixedBucketSampler`, but the validation dataset doesn't require the sampler."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"C:\\Users\\Zhiming\\Miniconda3\\envs\\mx\\lib\\site-packages\\gluonnlp\\data\\batchify\\batchify.py:228: UserWarning: Padding value is not given and will be set automatically to 0 in data.batchify.Pad(). Please check whether this is intended (e.g. value of padding index in the vocabulary).\n  'Padding value is not given and will be set automatically to 0 '\nFixedBucketSampler:\n  sample_num=4750, batch_num=153\n  key=[57, 84, 111, 138, 165, 192, 219, 246, 273, 300]\n  cnt=[35, 59, 51, 90, 40, 44, 67, 14, 11, 4339]\n  batch_size=[84, 57, 43, 34, 32, 32, 32, 32, 32, 32]\n"}],"source":"batch_size = 32\nbucket_num = 10\nbucket_ratio = 0.5\n\n\ndef get_dataloader():\n    # Construct the DataLoader pad data, stack label and lengths\n    batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(axis=0), \\\n                                          nlp.data.batchify.Pad(axis=0),\n                                          nlp.data.batchify.Stack())\n\n    # n this example, we use a FixedBucketSampler,\n    # which assigns each data sample to a fixed bucket based on its length.\n    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n        train_data_lengths,\n        batch_size=batch_size,\n        num_buckets=bucket_num,\n        ratio=bucket_ratio,\n        shuffle=True)\n    print(batch_sampler.stats())\n\n    # train_dataloader\n    train_dataloader = gluon.data.DataLoader(\n        dataset=train_dataset,\n        batch_sampler=batch_sampler,\n        batchify_fn=batchify_fn)\n    # valid_dataloader\n    valid_dataloader = gluon.data.DataLoader(\n        dataset=valid_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        batchify_fn=batchify_fn)\n    return train_dataloader, valid_dataloader\n\ntrain_dataloader, valid_dataloader = get_dataloader()"},{"cell_type":"markdown","metadata":{"scrolled":true},"source":["### Weighted softmax cross entropy loss\n","For a regular cross entropy loss, it is calculated by $-\\vec{y}^\\mathsf{T}\\log \\hat{\\vec{y}}$. Since $\\vec{y}$ is actually one-hot, so the value is $-\\log\\hat{y}_i$ where $i$ is the true label's index. To apply weight on this label, we multiply it with some weight value $w_i$, therefore, the loss of class $i$ is $-w_i\\log\\hat{y}_i$."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":"class WeightedSoftmaxCE(nn.HybridBlock):\n    def __init__(self, sparse_label=True, from_logits=False,  **kwargs):\n        super(WeightedSoftmaxCE, self).__init__(**kwargs)\n        with self.name_scope():\n            self.sparse_label = sparse_label\n            self.from_logits = from_logits\n\n    def hybrid_forward(self, F, pred, label, class_weight, depth=None):\n        if self.sparse_label:\n            label = F.reshape(label, shape=(-1, ))\n            label = F.one_hot(label, depth)\n        if not self.from_logits:\n            pred = F.log_softmax(pred, -1)\n\n        weight_label = F.broadcast_mul(label, class_weight)\n        loss = -F.sum(pred * weight_label, axis=-1)\n        \n        return loss"},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":"class AlignAttention(nn.Block):\n    def __init__(self, **kwargs):\n        super(AlignAttention, self).__init__(**kwargs)\n    \n    def forward(self, inp_left, inp_right):\n        # input dimension is of (batch_size, seq_len, embed_size)\n        # att dimension is of (batch_size, seq_len_left, seq_len_right)\n        att = nd.batch_dot(inp_left, nd.transpose(inp_right, axes = (0, 2, 1)))\n        # inp_left_dot dimention is of (batch_size, seq_left, embed_size)\n        inp_left_dot = nd.batch_dot(nd.softmax(att, axis=-1), inp_right)\n        # inp_right_dot dimension is of (batch_size, seq_right, embed_size)\n        inp_right_dot = nd.batch_dot(nd.softmax(nd.transpose(att, axes=(0, 2, 1)), axis=-1), inp_left)\n        # concat original (lstm output, dot multiplier, substraction, elementwise product)\n        # therefore, the real size is (batch_size, seq_len_left/right, embed_size*4)\n        aug_left = nd.concat(inp_left, inp_left_dot, inp_left-inp_left_dot, inp_left*inp_left_dot, dim=-1)\n        aug_right = nd.concat(inp_right, inp_right_dot, inp_right-inp_right_dot, inp_right*inp_right_dot, dim=-1)\n        return aug_left, aug_right, att"},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":"class EnhancedSeqInfer(nn.Block):\n    def __init__(self, vocab_len, embsize, nhidden, nlayers, nfc, nclass, drop_prob, **kwargs):\n        super(EnhancedSeqInfer, self).__init__(**kwargs)\n        with self.name_scope():\n            # dropout prob\n            self.drop_prob = drop_prob\n            # word embedding\n            self.embedding_layer = nn.Embedding(vocab_len, embsize)\n            # first lstm, from sentence embed to hidden outputs\n            self.bilstm1 = rnn.LSTM(nhidden, num_layers=nlayers, dropout=drop_prob, bidirectional=True)\n            # second lstm, from augmented embed to m\n            self.bilstm2 = rnn.LSTM(nhidden, num_layers=1, dropout=drop_prob, bidirectional=True)\n            # enhancement\n            self.align_att = AlignAttention()\n            # this layer is used to output the final class\n            self.output_layer = nn.HybridSequential()\n            self.output_layer.add(nn.Dense(nfc, activation='tanh'), nn.Dropout(rate=drop_prob), \\\n                                  nn.Dense(nfc, activation='tanh'), nn.Dropout(rate=drop_prob), \\\n                                  nn.Dense(nclass))\n\n    def forward(self, inp_left, inp_right):\n        # inp is a list containing left_text and right_text\n        # their size: [batch, token_idx]\n        # inp_embed_left/right size: [batch, seq_len, embed_size]\n        inp_embed_left = self.embedding_layer(inp_left)\n        inp_embed_right = self.embedding_layer(inp_right)\n        # rnn requires the first dimension to be the time steps, output is (seq_len, batch_size, embed_size)\n        h_output_left = self.bilstm1(nd.transpose(inp_embed_left, axes=(1, 0, 2)))\n        h_output_right = self.bilstm1(nd.transpose(inp_embed_right, axes=(1, 0, 2)))\n        m_left, m_right, att = self.align_att(nd.transpose(h_output_left, axes=(1, 0, 2)), \\\n                                                      nd.transpose(h_output_right, axes=(1, 0, 2)))\n        # apply another layer of lstm\n        # v_left/right shape is (seq_len, batch_size, embed_size)\n        v_left = self.bilstm2(nd.transpose(m_left, axes=(1, 0, 2)))\n        v_right = self.bilstm2(nd.transpose(m_right, axes=(1, 0, 2)))\n        # restore v's shape (batch_size, seq_len, embed_size)\n        v_left = nd.transpose(v_left, axes=(1, 0, 2))\n        v_right = nd.transpose(v_right, axes=(1, 0, 2))\n        # apply max pooling 1D and avg pooling 1D\n        v_left_avg = nd.sum(v_left, axis=1) / v_left.shape[1]\n        v_right_avg = nd.sum(v_right, axis=1) / v_right.shape[1]\n        v_left_max = nd.max(v_left, axis=1)\n        v_right_max = nd.max(v_right, axis=1)\n        # concatenate these 4 matrices\n        dense_input = nd.concat(v_left_avg, v_left_max, v_right_avg, v_left_max, dim=-1)\n        \n        output = self.output_layer(dense_input)\n        return output, att"},{"cell_type":"markdown","metadata":{},"source":["### Configure parameters and build models\n","`test_model` is set to `True` if we want to just load parameters and test on the model. Otherwise, it is set to `False` during training."]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"EnhancedSeqInfer(\n  (embedding_layer): Embedding(40004 -> 300, float32)\n  (bilstm1): LSTM(None -> 300, TNC, num_layers=2, dropout=0.5, bidirectional)\n  (bilstm2): LSTM(None -> 300, TNC, dropout=0.5, bidirectional)\n  (align_att): AlignAttention(\n  \n  )\n  (output_layer): HybridSequential(\n    (0): Dense(None -> 256, Activation(tanh))\n    (1): Dropout(p = 0.5, axes=())\n    (2): Dense(None -> 256, Activation(tanh))\n    (3): Dropout(p = 0.5, axes=())\n    (4): Dense(None -> 2, linear)\n  )\n)\n"}],"source":"test_model = False\nvocab_len = len(indexer.vocab)\nemsize = 300    # word embedding size\nnhidden = 300   # lstm hidden_dim\nnlayers = 2     # lstm layers\n\n# final fc layer's number of hidden units and predicted number of classes\nnfc = 256\nnclass = 2\n\ndrop_prob = 0.5\n\nctx = mx.gpu(0)\n\nmodel = EnhancedSeqInfer(vocab_len, emsize, nhidden, nlayers, nfc, nclass, drop_prob)\n\nif test_model:\n    model.load_parameters('model/esim-0.79.params', ctx=ctx)\nelse:\n    model.initialize(init=init.Xavier(), ctx=ctx)\n# Attach a pre-trained glove word vector to the embedding layer\nmodel.embedding_layer.weight.set_data(indexer.vocab.embedding.idx_to_vec)\n# fixed the embedding layer\nmodel.embedding_layer.collect_params().setattr('grad_req', 'null')\n\nprint(model)\n\ntrain_curve, valid_curve = [], []"},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":"def calculate_loss(x_left, x_right, y, model, loss, class_weight):\n    pred, att = model(x_left, x_right)\n    y = nd.array(y.astype('int32', copy=False), ctx=ctx)\n    if loss_name == 'sce':\n        l = loss(pred, y)\n    elif loss_name == 'wsce':\n        l = loss(pred, y, class_weight, class_weight.shape[0])\n    else:\n        raise NotImplemented\n    return pred, l"},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":"def one_epoch(data_iter, model, loss, trainer, ctx, is_train, epoch, class_weight=None, loss_name='sce'):\n\n    loss_val = 0.\n    total_pred = []\n    total_true = []\n    n_batch = 0\n\n    for batch_x_left, batch_x_right, batch_y in data_iter:\n        batch_x_left = batch_x_left.as_in_context(ctx)\n        batch_x_right = batch_x_right.as_in_context(ctx)\n        batch_y = batch_y.as_in_context(ctx)\n\n        if is_train:\n            with autograd.record():\n                batch_pred, l = calculate_loss(batch_x_left, batch_x_right, \\\n                                               batch_y, model, loss, class_weight)\n\n            # backward calculate\n            l.backward()\n\n            # update parmas\n            trainer.step(batch_x_left.shape[0])\n\n        else:\n            batch_pred, l = calculate_loss(batch_x_left, batch_x_right, \\\n                                           batch_y, model, loss, class_weight)\n\n        # keep result for metric\n        batch_pred = nd.argmax(nd.softmax(batch_pred, axis=1), axis=1).asnumpy()\n        batch_true = np.reshape(batch_y.asnumpy(), (-1, ))\n        total_pred.extend(batch_pred.tolist())\n        total_true.extend(batch_true.tolist())\n        \n        batch_loss = l.mean().asscalar()\n\n        n_batch += 1\n        loss_val += batch_loss\n\n        # check the result of traing phase\n        if is_train and n_batch % 400 == 0:\n            print('epoch %d, batch %d, batch_train_loss %.4f, batch_train_acc %.3f' %\n                  (epoch, n_batch, batch_loss, accuracy_score(batch_true, batch_pred)))\n\n    # metric\n    F1 = f1_score(np.array(total_true), np.array(total_pred), average='binary')\n    acc = accuracy_score(np.array(total_true), np.array(total_pred))\n    loss_val /= n_batch\n\n    if is_train:\n        print('epoch %d, learning_rate %.5f \\n\\t train_loss %.4f, acc_train %.3f, F1_train %.3f, ' %\n              (epoch, trainer.learning_rate, loss_val, acc, F1))\n        train_curve.append((acc, F1))\n        # declay lr\n        if epoch % 2 == 0:\n            trainer.set_learning_rate(trainer.learning_rate * 0.9)\n    else:\n        print('\\t valid_loss %.4f, acc_valid %.3f, F1_valid %.3f, ' % (loss_val, acc, F1))\n        valid_curve.append((acc, F1))"},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":"def train_valid(data_iter_train, data_iter_valid, model, loss, trainer, \\\n                ctx, nepochs, class_weight=None, loss_name='sce'):\n\n    for epoch in range(1, nepochs+1):\n        start = time.time()\n        # train\n        is_train = True\n        one_epoch(data_iter_train, model, loss, trainer, ctx, is_train, epoch, class_weight, loss_name)\n\n        # valid\n        is_train = False\n        one_epoch(data_iter_valid, model, loss, trainer, ctx, is_train, epoch, class_weight, loss_name)\n        end = time.time()\n        print('time %.2f sec' % (end-start))\n        print(\"*\"*100)"},{"cell_type":"markdown","metadata":{},"source":["### Train"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":"class_weight = None\nloss_name = 'sce'\noptim_name = 'adam'\nlr = 0.001\nclip = 2.5\nnepochs = 10\n\ntrainer = gluon.Trainer(model.collect_params(), optim_name, {'learning_rate': lr, 'clip_gradient': clip})\n\nif loss_name == 'sce':\n    loss = gluon.loss.SoftmaxCrossEntropyLoss()\nelif loss_name == 'wsce':\n    loss = WeightedSoftmaxCE()\n    # the value of class_weight is obtained by counting data in advance. It can be seen as a hyperparameter.\n    class_weight = nd.array([1., 3.], ctx=ctx)\nelse:\n    print('loss function {} is not implemented!'.format(loss_name))\n    raise NotImplemented"},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"epoch 1, learning_rate 0.00100 \n\t train_loss 0.5867, acc_train 0.720, F1_train 0.151,\nC:\\Users\\Zhiming\\Miniconda3\\envs\\mx\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n  'precision', 'predicted', average, warn_for)\n\t valid_loss 0.5444, acc_valid 0.752, F1_valid 0.000,\ntime 140.06 sec\n****************************************************************************************************\nepoch 2, learning_rate 0.00100 \n\t train_loss 0.5616, acc_train 0.735, F1_train 0.169,\n\t valid_loss 0.5235, acc_valid 0.740, F1_valid 0.330,\ntime 139.72 sec\n****************************************************************************************************\n"},{"ename":"MXNetError","evalue":"[22:56:29] c:\\jenkins\\workspace\\mxnet-tag\\mxnet\\src\\operator\\./rnn-inl.h:781: Check failed: e == CUDNN_STATUS_SUCCESS (8 vs. 0) : cuDNN: CUDNN_STATUS_EXECUTION_FAILED","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mMXNetError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32m<ipython-input-19-3f359aa286e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# train and valid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m train_valid(train_dataloader, valid_dataloader, model, loss, \\\n\u001b[1;32m----> 3\u001b[1;33m             trainer, ctx, nepochs, class_weight=class_weight, loss_name=loss_name)\n\u001b[0m","\u001b[1;32m<ipython-input-17-94bd8d4a43cb>\u001b[0m in \u001b[0;36mtrain_valid\u001b[1;34m(data_iter_train, data_iter_valid, model, loss, trainer, ctx, nepochs, class_weight, loss_name)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mis_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mone_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_iter_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# valid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<ipython-input-16-849f1cb604e5>\u001b[0m in \u001b[0;36mone_epoch\u001b[1;34m(data_iter, model, loss, trainer, ctx, is_train, epoch, class_weight, loss_name)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# keep result for metric\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mbatch_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mbatch_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mtotal_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Miniconda3\\envs\\mx\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1994\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1995\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1996\u001b[1;33m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[0;32m   1997\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Miniconda3\\envs\\mx\\lib\\site-packages\\mxnet\\base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    251\u001b[0m     \"\"\"\n\u001b[0;32m    252\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mMXNetError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMXGetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mMXNetError\u001b[0m: [22:56:29] c:\\jenkins\\workspace\\mxnet-tag\\mxnet\\src\\operator\\./rnn-inl.h:781: Check failed: e == CUDNN_STATUS_SUCCESS (8 vs. 0) : cuDNN: CUDNN_STATUS_EXECUTION_FAILED"]}],"source":"# train and valid\ntrain_valid(train_dataloader, valid_dataloader, model, loss, \\\n            trainer, ctx, nepochs, class_weight=class_weight, loss_name=loss_name)"},{"cell_type":"markdown","metadata":{},"source":["### Save the model and the training, validation curves"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-1-435e70efc0e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model/esim-{:.4}.params'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_curve\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'acc_record'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_curve\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_curve\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":"model.save_parameters('model/esim-{:.4}.params'.format(str(valid_curve[-1][0])))\nwith open('acc_record', 'w') as f:\n    f.write(str(train_curve)+'\\n')\n    f.write(str(valid_curve))"},{"cell_type":"markdown","metadata":{},"source":["### Visualize the training and validation curves\n","We can find in the plot that after how many epochs the model performs best on validation set without overfitting."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"from ast import literal_eval\nwith open('acc_record', 'r') as f:\n    train_curve, valid_curve = [ literal_eval(line) for line in f]"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"train_acc = [ acc for acc, _ in train_curve ]\ntrain_f1 = [ f1 for _, f1 in train_curve ]\nvalid_acc = [ acc for acc, _ in valid_curve ]\nvalid_f1 = [ f1 for _, f1 in valid_curve ]\nepochs = [ i for i in range(1, len(valid_curve)+1) ]\nimport matplotlib.pyplot as plt\nplt.title('Accuracy')\nplt.plot(epochs, train_acc, label='train', color='orange')\nplt.plot(epochs, valid_acc, label='validation', color='green')\nplt.legend(loc='upper left')\nplt.show()\nplt.title('F1')\nplt.plot(epochs, train_f1, label='train', color='orange')\nplt.plot(epochs, valid_f1, label='validation', color='green')\nplt.legend(loc='upper left')\nplt.show()"},{"cell_type":"markdown","metadata":{},"source":["### Test on randomly made up plot and review by myself\n","Now we will randomly input a movie plot summary and its review to see if the model decide the review is a spoiler or not."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"right = 'Tim killed Tom on a winter evening, and Sarah saw this scene.'\nleft = 'Sarah saw Tom killed, it is Tim to blame'\nleft_token = indexer.vocab[tokenizer.tokenizer(left)]\nright_token = indexer.vocab[tokenizer.tokenizer(right)]"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"left_input = nd.array(left_token, ctx=ctx).reshape(1,-1)\nright_input = nd.array(right_token, ctx=ctx).reshape(1,-1)\npred, att = model(left_input, right_input)\npred, nd.argmax(pred, axis=1).asscalar()"},{"cell_type":"markdown","metadata":{},"source":["### Visualize the soft alignment"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false},"outputs":[],"source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nvis_att = np.squeeze(att.asnumpy())\nplt.figure(figsize=vis_att.T.shape)\nsns.heatmap(vis_att, xticklabels=tokenizer.tokenizer(right), yticklabels=tokenizer.tokenizer(left), center=0.)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}