{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Sequence Inference Model\n",
    "The model is based on [this paper](https://arxiv.org/abs/1609.06038).\n",
    "\n",
    "### Import Related Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import multiprocessing as mp\n",
    "import gluonnlp as nlp\n",
    "\n",
    "from mxnet import gluon, nd, init\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet import autograd, gluon, nd\n",
    "from d2l import try_gpu\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn's metric function to evaluate the results of the experiment\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# fixed random number seed\n",
    "np.random.seed(9102)\n",
    "mx.random.seed(9102)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline\n",
    "\n",
    "### Load Dataset\n",
    "\n",
    "See [dataloader](data_loader.ipynb) for how the training samples are obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = 'data/imdb/'\n",
    "file_name = 'train.csv'\n",
    "file_path = data_folder + file_name\n",
    "# dev: True - only use a small dataset\n",
    "# create_vocab: True - create a new vocabulary from training data\n",
    "dev = True\n",
    "# load train file\n",
    "if dev:\n",
    "    # load only n rows\n",
    "    nrows = 10000\n",
    "    data = pd.read_csv(file_path, nrows=nrows)\n",
    "else:\n",
    "    # load as many as possible\n",
    "    data = pd.read_csv(file_path)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 3),                                            sentence1  \\\n",
       " 0  Inside a snowflake exists the magical land of ...   \n",
       " 1  In an ancient time, predating the pyramids, th...   \n",
       " 2  James Bond goes on his first ever mission as a...   \n",
       " 3  The Lost City of Z tells the incredible true s...   \n",
       " 4  This is the tale of Harry Potter, an ordinary ...   \n",
       " \n",
       "                                            sentence2  label  \n",
       " 0  Great one but doesn't set up there with the or...  False  \n",
       " 1  TSK: A Mildly Amusing Romp...But No People's E...  False  \n",
       " 2  Why all the accolades? I have to tell you that...  False  \n",
       " 3  Movie fell down I read this book many years ag...  False  \n",
       " 4  Gryffindor 150, Muggle Director 0 After readin...  False  )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(inplace=True)\n",
    "data.shape, data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2num = {'contradiction':0, 'neutral':1, 'entailment':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9500, 500)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list of review a label paris.\n",
    "dataset = [[left, right, label] for left, right, label in \\\n",
    "           zip(data['sentence1'], data['sentence2'], data['label']) if label!='-' ]\n",
    "# randomly divide one percent from the training set as a verification set.\n",
    "train_dataset, valid_dataset = nlp.data.train_valid_split(dataset)\n",
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A rare atmospheric phenomenon allows a New York City firefighter to communicate with his son 30 years in the future via HAM radio. The son uses this opportunity to warn the father of his impending death in a warehouse fire, and manages to save his life. However, what he does not realize is that changing history has triggered a new set of tragic events, including the murder of his mother. The two men must now work together, 30 years apart, to find the murderer before he strikes so that they can change history--again.',\n",
       " \"Probably the Best Time Travel Film Ever This one really surprised me. Going into Frequency, I had no idea that filmmakers today can still deal with the treacherous issue of time travel in a manner that doesn't insult my intelligence. This is a well written, well crafted story based on a premise that is theoretically possible. There were scenes where I actually had a chill run down my spine. It's not perfect (there's no clear explanation as to how the killer arrived in the hospital bed in 1969 where he was supposed to have died, and I had a serious problem with the fake NY accents from the get-go), but overall it works and it works amazingly well. Probably one of the 2 or 3 best films of 2000.\",\n",
       " False]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "\n",
    "The purpose of the following code is to process the raw data so that the processed data can be used for model training and prediction. We will use the `SpacyTokenizer` to split the document into tokens, `ClipSequence` to crop the comments to the specified length, and build a vocabulary based on the word frequency of the training data. Then we attach the [Glove](https://nlp.stanford.edu/pubs/glove.pdf)[2]  pre-trained word vector to the vocabulary and convert each token into the corresponding word index in the vocabulary.\n",
    "Finally get the standardized training data set and verification data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Sequence clipping and get length Time=40.99s, #Sentences=9500\n",
      "Done! Sequence clipping and get length Time=34.85s, #Sentences=500\n"
     ]
    }
   ],
   "source": [
    "# tokenizer takes as input a string and outputs a list of tokens.\n",
    "tokenizer = nlp.data.SpacyTokenizer('en')\n",
    "length_review = 300\n",
    "length_plot = 300\n",
    "\n",
    "from src.util import mp_tokenizer\n",
    "\n",
    "tokenizer = mp_tokenizer(tokenizer, length_review, length_plot)\n",
    "\n",
    "# Preprocess the dataset\n",
    "train_dataset_token, train_data_lengths = tokenizer.process_dataset(train_dataset)\n",
    "valid_dataset_token, valid_data_lengths = tokenizer.process_dataset(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.util import mp_indexer\n",
    "embedding = 'glove.42B.300d'\n",
    "indexer = mp_indexer(train_dataset_token, embedding)\n",
    "train_dataset = indexer.process_dataset(train_dataset_token)\n",
    "valid_dataset = indexer.process_dataset(valid_dataset_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([79,\n",
       "  51,\n",
       "  897,\n",
       "  5,\n",
       "  2643,\n",
       "  1524,\n",
       "  220,\n",
       "  7,\n",
       "  1602,\n",
       "  15244,\n",
       "  668,\n",
       "  9,\n",
       "  3602,\n",
       "  6,\n",
       "  12,\n",
       "  38,\n",
       "  662,\n",
       "  9,\n",
       "  10560,\n",
       "  1602,\n",
       "  16,\n",
       "  25,\n",
       "  4005,\n",
       "  4898,\n",
       "  25,\n",
       "  5,\n",
       "  220,\n",
       "  465,\n",
       "  46,\n",
       "  8,\n",
       "  7375,\n",
       "  6,\n",
       "  123,\n",
       "  4,\n",
       "  7375,\n",
       "  5455,\n",
       "  1526,\n",
       "  71,\n",
       "  8,\n",
       "  1239,\n",
       "  9049,\n",
       "  1094,\n",
       "  5,\n",
       "  21,\n",
       "  2391,\n",
       "  1100,\n",
       "  10,\n",
       "  18,\n",
       "  7375,\n",
       "  2022,\n",
       "  68,\n",
       "  17621,\n",
       "  6,\n",
       "  574,\n",
       "  5,\n",
       "  5455,\n",
       "  2237,\n",
       "  82,\n",
       "  554,\n",
       "  19,\n",
       "  12898,\n",
       "  6,\n",
       "  5455,\n",
       "  16,\n",
       "  3809,\n",
       "  262,\n",
       "  220,\n",
       "  957,\n",
       "  1354,\n",
       "  23,\n",
       "  18,\n",
       "  2643,\n",
       "  9204,\n",
       "  6,\n",
       "  22,\n",
       "  4,\n",
       "  17249,\n",
       "  3869,\n",
       "  7,\n",
       "  42,\n",
       "  355,\n",
       "  10,\n",
       "  51,\n",
       "  167,\n",
       "  5,\n",
       "  5455,\n",
       "  1574,\n",
       "  9,\n",
       "  1583,\n",
       "  266,\n",
       "  16,\n",
       "  6028,\n",
       "  35,\n",
       "  163,\n",
       "  54,\n",
       "  4,\n",
       "  111,\n",
       "  16,\n",
       "  110,\n",
       "  9298,\n",
       "  1094,\n",
       "  6],\n",
       " [8,\n",
       "  63,\n",
       "  1249,\n",
       "  24,\n",
       "  20,\n",
       "  896,\n",
       "  10,\n",
       "  398,\n",
       "  37,\n",
       "  142,\n",
       "  49,\n",
       "  8,\n",
       "  24,\n",
       "  11,\n",
       "  160,\n",
       "  9,\n",
       "  40,\n",
       "  63,\n",
       "  6,\n",
       "  12,\n",
       "  110,\n",
       "  3254,\n",
       "  5,\n",
       "  37,\n",
       "  70,\n",
       "  26689,\n",
       "  487,\n",
       "  15,\n",
       "  37,\n",
       "  287,\n",
       "  12,\n",
       "  23,\n",
       "  8,\n",
       "  63,\n",
       "  67,\n",
       "  499,\n",
       "  4,\n",
       "  86,\n",
       "  540,\n",
       "  423,\n",
       "  6,\n",
       "  187,\n",
       "  11,\n",
       "  4,\n",
       "  428,\n",
       "  20,\n",
       "  25,\n",
       "  5455,\n",
       "  7,\n",
       "  95,\n",
       "  25,\n",
       "  6,\n",
       "  5455,\n",
       "  11,\n",
       "  38,\n",
       "  29244,\n",
       "  1094,\n",
       "  23,\n",
       "  345,\n",
       "  5,\n",
       "  7,\n",
       "  59,\n",
       "  30,\n",
       "  259,\n",
       "  49,\n",
       "  37,\n",
       "  57,\n",
       "  198,\n",
       "  9,\n",
       "  1827,\n",
       "  56,\n",
       "  52,\n",
       "  4,\n",
       "  1181,\n",
       "  7,\n",
       "  483,\n",
       "  56,\n",
       "  52,\n",
       "  6,\n",
       "  27,\n",
       "  18,\n",
       "  2022,\n",
       "  7,\n",
       "  109,\n",
       "  278,\n",
       "  969,\n",
       "  37,\n",
       "  128,\n",
       "  5,\n",
       "  7,\n",
       "  286,\n",
       "  278,\n",
       "  37,\n",
       "  605,\n",
       "  9,\n",
       "  1341,\n",
       "  17,\n",
       "  372,\n",
       "  19,\n",
       "  18752,\n",
       "  19439,\n",
       "  20,\n",
       "  4,\n",
       "  398,\n",
       "  10,\n",
       "  1649,\n",
       "  23,\n",
       "  55,\n",
       "  21,\n",
       "  11,\n",
       "  87,\n",
       "  8,\n",
       "  475,\n",
       "  3656,\n",
       "  7,\n",
       "  15813,\n",
       "  148,\n",
       "  35747,\n",
       "  7,\n",
       "  5839,\n",
       "  83,\n",
       "  4585,\n",
       "  12,\n",
       "  51,\n",
       "  3882,\n",
       "  851,\n",
       "  22,\n",
       "  4,\n",
       "  456,\n",
       "  39,\n",
       "  354,\n",
       "  5,\n",
       "  919,\n",
       "  6896,\n",
       "  5,\n",
       "  9,\n",
       "  357,\n",
       "  5455,\n",
       "  12,\n",
       "  1693,\n",
       "  5,\n",
       "  7,\n",
       "  561,\n",
       "  1180,\n",
       "  7,\n",
       "  1262,\n",
       "  9,\n",
       "  55,\n",
       "  11,\n",
       "  5,\n",
       "  12,\n",
       "  3822,\n",
       "  5,\n",
       "  8,\n",
       "  73,\n",
       "  582,\n",
       "  64,\n",
       "  6,\n",
       "  27,\n",
       "  4,\n",
       "  235,\n",
       "  302,\n",
       "  11,\n",
       "  5455,\n",
       "  5,\n",
       "  39,\n",
       "  12,\n",
       "  18,\n",
       "  167,\n",
       "  582,\n",
       "  25127,\n",
       "  106,\n",
       "  5,\n",
       "  461,\n",
       "  18,\n",
       "  106,\n",
       "  71,\n",
       "  161,\n",
       "  398,\n",
       "  5,\n",
       "  22,\n",
       "  21,\n",
       "  93,\n",
       "  20,\n",
       "  4,\n",
       "  15244,\n",
       "  143,\n",
       "  5,\n",
       "  313,\n",
       "  44,\n",
       "  10,\n",
       "  51,\n",
       "  253,\n",
       "  8,\n",
       "  104,\n",
       "  12577,\n",
       "  508,\n",
       "  23,\n",
       "  289,\n",
       "  332,\n",
       "  56,\n",
       "  6],\n",
       " 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketing and DataLoader\n",
    "Since each sentence may have a different length, we need to use `Pad` to fill the sentences in a minibatch to equal lengths so that the data can be quickly tensored on the GPU. At the same time, we need to use `Stack` to stack the category tags of a batch of data. For convenience, we use `Tuple` to combine `Pad` and `Stack`.\n",
    "\n",
    "In order to make the length of the sentence pad in each minibatch as small as possible, we should make the sentences with similar lengths in a batch as much as possible. In light of this, we consider constructing a sampler using `FixedBucketSampler`, which defines how the samples in a dataset will be iterated in a more economic way.\n",
    "\n",
    "Finally, we use `DataLoader` to build a data loader for the training. dataset and validation dataset. The training dataset requires `FixedBucketSampler`, but the validation dataset doesn't require the sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zhiming\\miniconda3\\envs\\mx\\lib\\site-packages\\gluonnlp\\data\\batchify\\batchify.py:228: UserWarning: Padding value is not given and will be set automatically to 0 in data.batchify.Pad(). Please check whether this is intended (e.g. value of padding index in the vocabulary).\n",
      "  'Padding value is not given and will be set automatically to 0 '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=9500, batch_num=269\n",
      "  key=[43, 63, 83, 103, 123, 143, 163, 183, 203, 223]\n",
      "  cnt=[160, 1072, 1796, 1360, 1139, 1222, 931, 547, 973, 300]\n",
      "  batch_size=[82, 56, 42, 34, 32, 32, 32, 32, 32, 32]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "bucket_num = 10\n",
    "bucket_ratio = 0.5\n",
    "\n",
    "\n",
    "def get_dataloader():\n",
    "    # Construct the DataLoader pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(axis=0), \\\n",
    "                                          nlp.data.batchify.Pad(axis=0),\n",
    "                                          nlp.data.batchify.Stack())\n",
    "\n",
    "    # n this example, we use a FixedBucketSampler,\n",
    "    # which assigns each data sample to a fixed bucket based on its length.\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_data_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "\n",
    "    # train_dataloader\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn)\n",
    "    # valid_dataloader\n",
    "    valid_dataloader = gluon.data.DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn)\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "train_dataloader, valid_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Weighted softmax cross entropy loss\n",
    "For a regular cross entropy loss, it is calculated by $-\\vec{y}^\\mathsf{T}\\log \\hat{\\vec{y}}$. Since $\\vec{y}$ is actually one-hot, so the value is $-\\log\\hat{y}_i$ where $i$ is the true label's index. To apply weight on this label, we multiply it with some weight value $w_i$, therefore, the loss of class $i$ is $-w_i\\log\\hat{y}_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSoftmaxCE(nn.HybridBlock):\n",
    "    def __init__(self, sparse_label=True, from_logits=False,  **kwargs):\n",
    "        super(WeightedSoftmaxCE, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.sparse_label = sparse_label\n",
    "            self.from_logits = from_logits\n",
    "\n",
    "    def hybrid_forward(self, F, pred, label, class_weight, depth=None):\n",
    "        if self.sparse_label:\n",
    "            label = F.reshape(label, shape=(-1, ))\n",
    "            label = F.one_hot(label, depth)\n",
    "        if not self.from_logits:\n",
    "            pred = F.log_softmax(pred, -1)\n",
    "\n",
    "        weight_label = F.broadcast_mul(label, class_weight)\n",
    "        loss = -F.sum(pred * weight_label, axis=-1)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignAttention(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AlignAttention, self).__init__(**kwargs)\n",
    "    \n",
    "    def forward(self, inp_left, inp_right):\n",
    "        # input dimension is of (batch_size, seq_len, embed_size)\n",
    "        # att dimension is of (batch_size, seq_len_left, seq_len_right)\n",
    "        att = nd.batch_dot(inp_left, nd.transpose(inp_right, axes = (0, 2, 1)))\n",
    "        # inp_left_dot dimention is of (batch_size, seq_left, embed_size)\n",
    "        inp_left_dot = nd.batch_dot(nd.softmax(att, axis=-1), inp_right)\n",
    "        # inp_right_dot dimension is of (batch_size, seq_right, embed_size)\n",
    "        inp_right_dot = nd.batch_dot(nd.softmax(nd.transpose(att, axes=(0, 2, 1)), axis=-1), inp_left)\n",
    "        # concat original (lstm output, dot multiplier, substraction, elementwise product)\n",
    "        # therefore, the real size is (batch_size, seq_len_left/right, embed_size*4)\n",
    "        aug_left = nd.concat(inp_left, inp_left_dot, inp_left-inp_left_dot, inp_left*inp_left_dot, dim=-1)\n",
    "        aug_right = nd.concat(inp_right, inp_right_dot, inp_right-inp_right_dot, inp_right*inp_right_dot, dim=-1)\n",
    "        return aug_left, aug_right, att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedSeqInfer(nn.Block):\n",
    "    def __init__(self, vocab_len, embsize, nhidden, nlayers, nfc, nclass, drop_prob, **kwargs):\n",
    "        super(EnhancedSeqInfer, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # dropout prob\n",
    "            self.drop_prob = drop_prob\n",
    "            # word embedding\n",
    "            self.embedding_layer = nn.Embedding(vocab_len, embsize)\n",
    "            # first lstm, from sentence embed to hidden outputs\n",
    "            self.bilstm1 = rnn.LSTM(nhidden, num_layers=nlayers, dropout=drop_prob, bidirectional=True)\n",
    "            # second lstm, from augmented embed to m\n",
    "            self.bilstm2 = rnn.LSTM(nhidden, num_layers=1, dropout=drop_prob, bidirectional=True)\n",
    "            # enhancement\n",
    "            self.align_att = AlignAttention()\n",
    "            # this layer is used to output the final class\n",
    "            self.output_layer = nn.HybridSequential()\n",
    "            self.output_layer.add(nn.Dense(nfc, activation='tanh'), nn.Dropout(rate=drop_prob), \\\n",
    "                                  nn.Dense(nfc, activation='tanh'), nn.Dropout(rate=drop_prob), \\\n",
    "                                  nn.Dense(nclass))\n",
    "\n",
    "    def forward(self, inp_left, inp_right):\n",
    "        # inp is a list containing left_text and right_text\n",
    "        # their size: [batch, token_idx]\n",
    "        # inp_embed_left/right size: [batch, seq_len, embed_size]\n",
    "        inp_embed_left = self.embedding_layer(inp_left)\n",
    "        inp_embed_right = self.embedding_layer(inp_right)\n",
    "        # rnn requires the first dimension to be the time steps, output is (seq_len, batch_size, embed_size)\n",
    "        h_output_left = self.bilstm1(nd.transpose(inp_embed_left, axes=(1, 0, 2)))\n",
    "        h_output_right = self.bilstm1(nd.transpose(inp_embed_right, axes=(1, 0, 2)))\n",
    "        m_left, m_right, att = self.align_att(nd.transpose(h_output_left, axes=(1, 0, 2)), \\\n",
    "                                                      nd.transpose(h_output_right, axes=(1, 0, 2)))\n",
    "        # apply another layer of lstm\n",
    "        # v_left/right shape is (seq_len, batch_size, embed_size)\n",
    "        v_left = self.bilstm2(nd.transpose(m_left, axes=(1, 0, 2)))\n",
    "        v_right = self.bilstm2(nd.transpose(m_right, axes=(1, 0, 2)))\n",
    "        # restore v's shape (batch_size, seq_len, embed_size)\n",
    "        v_left = nd.transpose(v_left, axes=(1, 0, 2))\n",
    "        v_right = nd.transpose(v_right, axes=(1, 0, 2))\n",
    "        # apply max pooling 1D and avg pooling 1D\n",
    "        v_left_avg = nd.sum(v_left, axis=1) / v_left.shape[1]\n",
    "        v_right_avg = nd.sum(v_right, axis=1) / v_right.shape[1]\n",
    "        v_left_max = nd.max(v_left, axis=1)\n",
    "        v_right_max = nd.max(v_right, axis=1)\n",
    "        # concatenate these 4 matrices\n",
    "        dense_input = nd.concat(v_left_avg, v_left_max, v_right_avg, v_left_max, dim=-1)\n",
    "        \n",
    "        output = self.output_layer(dense_input)\n",
    "        return output, att"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure parameters and build models\n",
    "`test_model` is set to `True` if we want to just load parameters and test on the model. Otherwise, it is set to `False` during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnhancedSeqInfer(\n",
      "  (embedding_layer): Embedding(40004 -> 300, float32)\n",
      "  (bilstm1): LSTM(None -> 300, TNC, num_layers=2, dropout=0.5, bidirectional)\n",
      "  (bilstm2): LSTM(None -> 300, TNC, dropout=0.5, bidirectional)\n",
      "  (align_att): AlignAttention(\n",
      "  \n",
      "  )\n",
      "  (output_layer): HybridSequential(\n",
      "    (0): Dense(None -> 512, Activation(tanh))\n",
      "    (1): Dropout(p = 0.5, axes=())\n",
      "    (2): Dense(None -> 512, Activation(tanh))\n",
      "    (3): Dropout(p = 0.5, axes=())\n",
      "    (4): Dense(None -> 2, linear)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "test_model = False\n",
    "vocab_len = len(indexer.vocab)\n",
    "emsize = 300    # word embedding size\n",
    "nhidden = 300   # lstm hidden_dim\n",
    "nlayers = 2     # lstm layers\n",
    "\n",
    "# final fc layer's number of hidden units and predicted number of classes\n",
    "nfc = 512\n",
    "nclass = 2\n",
    "\n",
    "drop_prob = 0.5\n",
    "\n",
    "ctx = mx.gpu(0)\n",
    "\n",
    "model = EnhancedSeqInfer(vocab_len, emsize, nhidden, nlayers, nfc, nclass, drop_prob)\n",
    "\n",
    "if test_model:\n",
    "    model.load_parameters('model/esim-0.79.params', ctx=ctx)\n",
    "else:\n",
    "    model.initialize(init=init.Xavier(), ctx=ctx)\n",
    "# Attach a pre-trained glove word vector to the embedding layer\n",
    "model.embedding_layer.weight.set_data(indexer.vocab.embedding.idx_to_vec)\n",
    "# fixed the embedding layer\n",
    "model.embedding_layer.collect_params().setattr('grad_req', 'null')\n",
    "\n",
    "print(model)\n",
    "\n",
    "train_curve, valid_curve = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(x_left, x_right, y, model, loss, class_weight):\n",
    "    pred, att = model(x_left, x_right)\n",
    "    y = nd.array(y.asnumpy().astype('int32')).as_in_context(ctx)\n",
    "    if loss_name == 'sce':\n",
    "        l = loss(pred, y)\n",
    "    elif loss_name == 'wsce':\n",
    "        l = loss(pred, y, class_weight, class_weight.shape[0])\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "    return pred, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(data_iter, model, loss, trainer, ctx, is_train, epoch, class_weight=None, loss_name='sce'):\n",
    "\n",
    "    loss_val = 0.\n",
    "    total_pred = []\n",
    "    total_true = []\n",
    "    n_batch = 0\n",
    "\n",
    "    for batch_x_left, batch_x_right, batch_y in data_iter:\n",
    "        batch_x_left = batch_x_left.as_in_context(ctx)\n",
    "        batch_x_right = batch_x_right.as_in_context(ctx)\n",
    "        batch_y = batch_y.as_in_context(ctx)\n",
    "\n",
    "        if is_train:\n",
    "            with autograd.record():\n",
    "                batch_pred, l = calculate_loss(batch_x_left, batch_x_right, \\\n",
    "                                               batch_y, model, loss, class_weight)\n",
    "\n",
    "            # backward calculate\n",
    "            l.backward()\n",
    "\n",
    "            # update parmas\n",
    "            trainer.step(batch_x_left.shape[0])\n",
    "\n",
    "        else:\n",
    "            batch_pred, l = calculate_loss(batch_x_left, batch_x_right, \\\n",
    "                                           batch_y, model, loss, class_weight)\n",
    "\n",
    "        # keep result for metric\n",
    "        batch_pred = nd.argmax(nd.softmax(batch_pred, axis=1), axis=1).asnumpy()\n",
    "        batch_true = np.reshape(batch_y.asnumpy(), (-1, ))\n",
    "        total_pred.extend(batch_pred.tolist())\n",
    "        total_true.extend(batch_true.tolist())\n",
    "        \n",
    "        batch_loss = l.mean().asscalar()\n",
    "\n",
    "        n_batch += 1\n",
    "        loss_val += batch_loss\n",
    "\n",
    "        # check the result of traing phase\n",
    "        if is_train and n_batch % 400 == 0:\n",
    "            print('epoch %d, batch %d, batch_train_loss %.4f, batch_train_acc %.3f' %\n",
    "                  (epoch, n_batch, batch_loss, accuracy_score(batch_true, batch_pred)))\n",
    "\n",
    "    # metric\n",
    "    F1 = f1_score(np.array(total_true), np.array(total_pred), average='binary')\n",
    "    acc = accuracy_score(np.array(total_true), np.array(total_pred))\n",
    "    loss_val /= n_batch\n",
    "\n",
    "    if is_train:\n",
    "        print('epoch %d, learning_rate %.5f \\n\\t train_loss %.4f, acc_train %.3f, F1_train %.3f, ' %\n",
    "              (epoch, trainer.learning_rate, loss_val, acc, F1))\n",
    "        train_curve.append((acc, F1))\n",
    "        # declay lr\n",
    "        if epoch % 2 == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * 0.9)\n",
    "    else:\n",
    "        print('\\t valid_loss %.4f, acc_valid %.3f, F1_valid %.3f, ' % (loss_val, acc, F1))\n",
    "        valid_curve.append((acc, F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid(data_iter_train, data_iter_valid, model, loss, trainer, \\\n",
    "                ctx, nepochs, class_weight=None, loss_name='sce'):\n",
    "\n",
    "    for epoch in range(1, nepochs+1):\n",
    "        start = time.time()\n",
    "        # train\n",
    "        is_train = True\n",
    "        one_epoch(data_iter_train, model, loss, trainer, ctx, is_train, epoch, class_weight, loss_name)\n",
    "\n",
    "        # valid\n",
    "        is_train = False\n",
    "        one_epoch(data_iter_valid, model, loss, trainer, ctx, is_train, epoch, class_weight, loss_name)\n",
    "        end = time.time()\n",
    "        print('time %.2f sec' % (end-start))\n",
    "        print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = None\n",
    "loss_name = 'sce'\n",
    "optim_name = 'adam'\n",
    "lr = 0.001\n",
    "clip = .5\n",
    "nepochs = 10\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), optim_name, {'learning_rate': lr, 'clip_gradient': clip})\n",
    "\n",
    "if loss_name == 'sce':\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "elif loss_name == 'wsce':\n",
    "    loss = WeightedSoftmaxCE()\n",
    "    # the value of class_weight is obtained by counting data in advance. It can be seen as a hyperparameter.\n",
    "    class_weight = nd.array([1., 3.], ctx=ctx)\n",
    "else:\n",
    "    print('loss function {} is not implemented!'.format(loss_name))\n",
    "    raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, learning_rate 0.00100 \n",
      "\t train_loss 0.5620, acc_train 0.732, F1_train 0.125, \n",
      "\t valid_loss 0.5313, acc_valid 0.718, F1_valid 0.390, \n",
      "time 135.45 sec\n",
      "****************************************************************************************************\n",
      "epoch 2, learning_rate 0.00100 \n",
      "\t train_loss 0.5395, acc_train 0.741, F1_train 0.158, \n",
      "\t valid_loss 0.5437, acc_valid 0.732, F1_valid 0.489, \n",
      "time 136.89 sec\n",
      "****************************************************************************************************\n",
      "epoch 3, learning_rate 0.00090 \n",
      "\t train_loss 0.5151, acc_train 0.758, F1_train 0.311, \n",
      "\t valid_loss 0.4882, acc_valid 0.788, F1_valid 0.535, \n",
      "time 136.50 sec\n",
      "****************************************************************************************************\n",
      "epoch 4, learning_rate 0.00090 \n",
      "\t train_loss 0.4997, acc_train 0.768, F1_train 0.353, \n",
      "\t valid_loss 0.5164, acc_valid 0.734, F1_valid 0.513, \n",
      "time 136.69 sec\n",
      "****************************************************************************************************\n",
      "epoch 5, learning_rate 0.00081 \n",
      "\t train_loss 0.4860, acc_train 0.778, F1_train 0.408, \n",
      "\t valid_loss 0.5008, acc_valid 0.776, F1_valid 0.509, \n",
      "time 137.61 sec\n",
      "****************************************************************************************************\n",
      "epoch 6, learning_rate 0.00081 \n",
      "\t train_loss 0.4653, acc_train 0.780, F1_train 0.431, \n",
      "\t valid_loss 0.5366, acc_valid 0.716, F1_valid 0.510, \n",
      "time 138.20 sec\n",
      "****************************************************************************************************\n",
      "epoch 7, learning_rate 0.00073 \n",
      "\t train_loss 0.4498, acc_train 0.790, F1_train 0.479, \n",
      "\t valid_loss 0.4860, acc_valid 0.766, F1_valid 0.435, \n",
      "time 137.04 sec\n",
      "****************************************************************************************************\n",
      "epoch 8, learning_rate 0.00073 \n",
      "\t train_loss 0.4327, acc_train 0.803, F1_train 0.527, \n",
      "\t valid_loss 0.5039, acc_valid 0.740, F1_valid 0.488, \n",
      "time 137.95 sec\n",
      "****************************************************************************************************\n",
      "epoch 9, learning_rate 0.00066 \n",
      "\t train_loss 0.4000, acc_train 0.820, F1_train 0.594, \n",
      "\t valid_loss 0.5294, acc_valid 0.730, F1_valid 0.483, \n",
      "time 136.24 sec\n",
      "****************************************************************************************************\n",
      "epoch 10, learning_rate 0.00066 \n",
      "\t train_loss 0.3805, acc_train 0.828, F1_train 0.621, \n",
      "\t valid_loss 0.5467, acc_valid 0.724, F1_valid 0.465, \n",
      "time 137.93 sec\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# train and valid\n",
    "train_valid(train_dataloader, valid_dataloader, model, loss, \\\n",
    "            trainer, ctx, nepochs, class_weight=class_weight, loss_name=loss_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model and the training, validation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_parameters('model/esim-{:.4}.params'.format(str(valid_curve[-1][0])))\n",
    "with open('acc_record', 'w') as f:\n",
    "    f.write(str(train_curve)+'\\n')\n",
    "    f.write(str(valid_curve))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the training and validation curves\n",
    "We can find in the plot that after how many epochs the model performs best on validation set without overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "with open('acc_record', 'r') as f:\n",
    "    train_curve, valid_curve = [ literal_eval(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = [ acc for acc, _ in train_curve ]\n",
    "train_f1 = [ f1 for _, f1 in train_curve ]\n",
    "valid_acc = [ acc for acc, _ in valid_curve ]\n",
    "valid_f1 = [ f1 for _, f1 in valid_curve ]\n",
    "epochs = [ i for i in range(1, len(valid_curve)+1) ]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Accuracy')\n",
    "plt.plot(epochs, train_acc, label='train', color='orange')\n",
    "plt.plot(epochs, valid_acc, label='validation', color='green')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "plt.title('F1')\n",
    "plt.plot(epochs, train_f1, label='train', color='orange')\n",
    "plt.plot(epochs, valid_f1, label='validation', color='green')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on randomly made up plot and review by myself\n",
    "Now we will randomly input a movie plot summary and its review to see if the model decide the review is a spoiler or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = 'today\\'s weather is great'\n",
    "left = 'the weather is good today'\n",
    "left_token = vocab[tokenizer(left)]\n",
    "right_token = vocab[tokenizer(right)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_input = nd.array(left_token, ctx=ctx).reshape(1,-1)\n",
    "right_input = nd.array(right_token, ctx=ctx).reshape(1,-1)\n",
    "pred, att = model(left_input, right_input)\n",
    "pred, nd.argmax(pred, axis=1).asscalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the soft alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "vis_att = np.squeeze(att.asnumpy())\n",
    "plt.figure(figsize=vis_att.T.shape)\n",
    "sns.heatmap(vis_att, xticklabels=tokenizer(right), yticklabels=tokenizer(left), center=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
