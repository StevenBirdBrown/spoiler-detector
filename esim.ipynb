{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Self-attentive Sentence Embedding\n",
    "\n",
    "The code is based on [gluon-nlp tutorial](https://gluon-nlp.mxnet.io/examples/sentence_embedding/self_attentive_sentence_embedding.html).\n",
    "\n",
    "## Import Related Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import multiprocessing as mp\n",
    "import gluonnlp as nlp\n",
    "\n",
    "from mxnet import gluon, nd, init\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet import autograd, gluon, nd\n",
    "from d2l import try_gpu\n",
    "import pandas as pd\n",
    "\n",
    "# iUse sklearn's metric function to evaluate the results of the experiment\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# fixed random number seed\n",
    "np.random.seed(2333)\n",
    "mx.random.seed(2333)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline\n",
    "\n",
    "### Load Dataset\n",
    "\n",
    "See [dataloader](data_loader.ipynb) for how the training samples are obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data/imdb/'\n",
    "file_name = 'train.csv'\n",
    "file_path = data_folder + file_name\n",
    "# dev: True - only use a small dataset\n",
    "# create_vocab: True - create a new vocabulary from training data\n",
    "dev = True\n",
    "# load train file\n",
    "if dev:\n",
    "    # load only n rows\n",
    "    nrows = 10000\n",
    "    data = pd.read_csv(file_path, nrows=nrows)\n",
    "else:\n",
    "    # load as many as possible\n",
    "    data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9500, 500)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list of review a label paris.\n",
    "dataset = [[left, right, int(label)] for left, right, label in \\\n",
    "           zip(data['review_text'], data['plot_summary'], data['is_spoiler'])]\n",
    "# randomly divide one percent from the training set as a verification set.\n",
    "train_dataset, valid_dataset = nlp.data.train_valid_split(dataset)\n",
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "\n",
    "The purpose of the following code is to process the raw data so that the processed data can be used for model training and prediction. We will use the `SpacyTokenizer` to split the document into tokens, `ClipSequence` to crop the comments to the specified length, and build a vocabulary based on the word frequency of the training data. Then we attach the [Glove](https://nlp.stanford.edu/pubs/glove.pdf)[2]  pre-trained word vector to the vocabulary and convert each token into the corresponding word index in the vocabulary.\n",
    "Finally get the standardized training data set and verification data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Sequence clipping and get length Time=4.39s, #Sentences=9500\n",
      "Done! Sequence clipping and get length Time=0.54s, #Sentences=500\n"
     ]
    }
   ],
   "source": [
    "# tokenizer takes as input a string and outputs a list of tokens.\n",
    "tokenizer = nlp.data.SpacyTokenizer('en_core_web_sm')\n",
    "\n",
    "# length_clip takes as input a list and outputs a list with maximum length 300.\n",
    "length_clip_review = nlp.data.ClipSequence(400)\n",
    "length_clip_plot = nlp.data.ClipSequence(200)\n",
    "\n",
    "def preprocess(x):\n",
    "    # now the first element in tuple is review, second plot and third label\n",
    "    left, right, label = x[0], x[1], int(x[2])\n",
    "    # clip the length of review words\n",
    "    left, right = length_clip_review(tokenizer(left.lower())), \\\n",
    "                  length_clip_plot(tokenizer(right.lower()))\n",
    "    return left, right, label\n",
    "\n",
    "def get_length(x):\n",
    "    return float(len(x[0]))\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    start = time.time()\n",
    "\n",
    "    with mp.Pool() as pool:\n",
    "        # Each sample is processed in an asynchronous manner.\n",
    "        dataset = gluon.data.ArrayDataset(pool.map(preprocess, dataset))\n",
    "        lengths = gluon.data.ArrayDataset(pool.map(get_length, dataset))\n",
    "    end = time.time()\n",
    "\n",
    "    print('Done! Sequence clipping and get length Time={:.2f}s, #Sentences={}'.format(end - start, len(dataset)))\n",
    "    return dataset, lengths\n",
    "\n",
    "# Preprocess the dataset\n",
    "train_dataset_token, train_data_lengths = preprocess_dataset(train_dataset)\n",
    "valid_dataset_token, valid_data_lengths = preprocess_dataset(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=40004, unk=\"<unk>\", reserved=\"['<pad>', '<bos>', '<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "# create vocab\n",
    "train_seqs = [ sample[0] + sample[1] for sample in train_dataset_token]\n",
    "counter = nlp.data.count_tokens(list(itertools.chain.from_iterable(train_seqs)))\n",
    "vocab = nlp.Vocab(counter, max_size=40000)\n",
    "\n",
    "# load pre-trained embedding, Glove\n",
    "embedding_weights = nlp.embedding.GloVe(source='glove.twitter.27B.200d')\n",
    "vocab.set_embedding(embedding_weights)\n",
    "print(vocab)\n",
    "\n",
    "# from string to token\n",
    "def token_to_idx(x):\n",
    "    return vocab[x[0]], vocab[x[1]], x[2]\n",
    "\n",
    "# A token index or a list of token indices is returned according to the vocabulary.\n",
    "with mp.Pool() as pool:\n",
    "    train_dataset = pool.map(token_to_idx, train_dataset_token)\n",
    "    valid_dataset = pool.map(token_to_idx, valid_dataset_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157, 233)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = 'still'\n",
    "vocab.embedding.token_to_idx[token], embedding_weights.token_to_idx[token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketing and DataLoader\n",
    "Since each sentence may have a different length, we need to use `Pad` to fill the sentences in a minibatch to equal lengths so that the data can be quickly tensored on the GPU. At the same time, we need to use `Stack` to stack the category tags of a batch of data. For convenience, we use `Tuple` to combine `Pad` and `Stack`.\n",
    "\n",
    "In order to make the length of the sentence pad in each minibatch as small as possible, we should make the sentences with similar lengths in a batch as much as possible. In light of this, we consider constructing a sampler using `FixedBucketSampler`, which defines how the samples in a dataset will be iterated in a more economic way.\n",
    "\n",
    "Finally, we use `DataLoader` to build a data loader for the training. dataset and validation dataset. The training dataset requires FixedBucketSampler, but the validation dataset doesn't require the sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=9500, batch_num=141\n",
      "  key=[58, 96, 134, 172, 210, 248, 286, 324, 362, 400]\n",
      "  cnt=[102, 465, 740, 1740, 1289, 846, 695, 609, 440, 2574]\n",
      "  batch_size=[220, 133, 95, 74, 64, 64, 64, 64, 64, 64]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "bucket_num = 10\n",
    "bucket_ratio = 0.5\n",
    "\n",
    "\n",
    "def get_dataloader():\n",
    "    # Construct the DataLoader pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(axis=0), \\\n",
    "                                          nlp.data.batchify.Pad(axis=0),\n",
    "                                          nlp.data.batchify.Stack())\n",
    "\n",
    "    # n this example, we use a FixedBucketSampler,\n",
    "    # which assigns each data sample to a fixed bucket based on its length.\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_data_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "\n",
    "    # train_dataloader\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn)\n",
    "    # valid_dataloader\n",
    "    valid_dataloader = gluon.data.DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn)\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "train_dataloader, valid_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# experiement one the two dataloaders\n",
    "# for left, right, label in train_dataloader:\n",
    "    # print(left.shape, right.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSoftmaxCE(nn.HybridBlock):\n",
    "    def __init__(self, sparse_label=True, from_logits=False,  **kwargs):\n",
    "        super(WeightedSoftmaxCE, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.sparse_label = sparse_label\n",
    "            self.from_logits = from_logits\n",
    "\n",
    "    def hybrid_forward(self, F, pred, label, class_weight, depth=None):\n",
    "        if self.sparse_label:\n",
    "            label = F.reshape(label, shape=(-1, ))\n",
    "            label = F.one_hot(label, depth)\n",
    "        if not self.from_logits:\n",
    "            pred = F.log_softmax(pred, -1)\n",
    "\n",
    "        weight_label = F.broadcast_mul(label, class_weight)\n",
    "        loss = -F.sum(pred * weight_label, axis=-1)\n",
    "\n",
    "        # return F.mean(loss, axis=0, exclude=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignAttention(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AlignAttention, self).__init__(**kwargs)\n",
    "    \n",
    "    def forward(self, inp_left, inp_right):\n",
    "        # input dimension is of (batch_size, seq_len, embed_size)\n",
    "        # att dimension is of (batch_size, seq_len_left, seq_len_right)\n",
    "        att = nd.batch_dot(inp_left, nd.transpose(inp_right, axes = (0, 2, 1)))\n",
    "        # inp_left_dot dimention is of (batch_size, seq_left, embed_size)\n",
    "        inp_left_dot = nd.batch_dot(nd.softmax(att, axis=-1), inp_right)\n",
    "        # inp_right_dot dimension is of (batch_size, seq_right, embed_size)\n",
    "        inp_right_dot = nd.batch_dot(nd.softmax(nd.transpose(att, axes=(0, 2, 1)), axis=-1), inp_left)\n",
    "        # concat original (lstm output, dot multiplier, substraction, elementwise product)\n",
    "        # therefore, the real size is (batch_size, seq_len_left/right, embed_size*4)\n",
    "        aug_left = nd.concat(inp_left, inp_left_dot, inp_left-inp_left_dot, inp_left*inp_left_dot, dim=-1)\n",
    "        aug_right = nd.concat(inp_right, inp_right_dot, inp_right-inp_right_dot, inp_right*inp_right_dot, dim=-1)\n",
    "        return aug_left, aug_right, att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedSeqInfer(nn.Block):\n",
    "    def __init__(self, vocab_len, embsize, nhidden, nlayers, nfc, nclass, drop_prob, **kwargs):\n",
    "        super(EnhancedSeqInfer, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # dropout prob\n",
    "            self.drop_prob = drop_prob\n",
    "            # word embedding\n",
    "            self.embedding_layer = nn.Embedding(vocab_len, embsize)\n",
    "            # first lstm, from sentence embed to hidden outputs\n",
    "            self.bilstm1 = rnn.LSTM(nhidden, num_layers=nlayers, dropout=drop_prob, bidirectional=True)\n",
    "            # second lstm, from augmented embed to m\n",
    "            self.bilstm2 = rnn.LSTM(nhidden, num_layers=1, dropout=drop_prob, bidirectional=True)\n",
    "            # enhancement\n",
    "            self.align_att = AlignAttention()\n",
    "            # this layer is used to output the final class\n",
    "            self.output_layer = nn.HybridSequential()\n",
    "            self.output_layer.add(nn.Dense(nfc, activation='tanh'), nn.Dropout(rate=drop_prob), \\\n",
    "                                  nn.Dense(nfc, activation='tanh'), nn.Dropout(rate=drop_prob), \\\n",
    "                                  nn.Dense(nclass))\n",
    "\n",
    "    def forward(self, inp_left, inp_right):\n",
    "        # inp is a list containing left_text and right_text\n",
    "        # their size: [batch, token_idx]\n",
    "        # inp_embed_left/right size: [batch, seq_len, embed_size]\n",
    "        inp_embed_left = self.embedding_layer(inp_left)\n",
    "        inp_embed_right = self.embedding_layer(inp_right)\n",
    "        # rnn requires the first dimension to be the time steps, output is (seq_len, batch_size, embed_size)\n",
    "        h_output_left = self.bilstm1(nd.transpose(inp_embed_left, axes=(1, 0, 2)))\n",
    "        h_output_right = self.bilstm1(nd.transpose(inp_embed_right, axes=(1, 0, 2)))\n",
    "        m_left, m_right, att = self.align_att(nd.transpose(h_output_left, axes=(1, 0, 2)), \\\n",
    "                                                      nd.transpose(h_output_right, axes=(1, 0, 2)))\n",
    "        # apply another layer of lstm\n",
    "        # v_left/right shape is (seq_len, batch_size, embed_size)\n",
    "        v_left = self.bilstm2(nd.transpose(m_left, axes=(1, 0, 2)))\n",
    "        v_right = self.bilstm2(nd.transpose(m_right, axes=(1, 0, 2)))\n",
    "        # restore v's shape (batch_size, seq_len, embed_size)\n",
    "        v_left = nd.transpose(v_left, axes=(1, 0, 2))\n",
    "        v_right = nd.transpose(v_right, axes=(1, 0, 2))\n",
    "        # apply max pooling 1D and avg pooling 1D\n",
    "        v_left_avg = nd.sum(v_left, axis=1) / v_left.shape[1]\n",
    "        v_right_avg = nd.sum(v_right, axis=1) / v_right.shape[1]\n",
    "        v_left_max = nd.max(v_left, axis=1)\n",
    "        v_right_max = nd.max(v_right, axis=1)\n",
    "        # concatenate these 4 matrices\n",
    "        dense_input = nd.concat(v_left_avg, v_left_max, v_right_avg, v_left_max, dim=-1)\n",
    "        \n",
    "        output = self.output_layer(dense_input)\n",
    "        return output, att"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure parameters and build models\n",
    "\n",
    "The resulting `M` is a matrix, and the way to classify this matrix is `flatten`, `mean` or `prune`. Prune is a way of trimming parameters proposed in the original paper and has been implemented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnhancedSeqInfer(\n",
      "  (embedding_layer): Embedding(40004 -> 200, float32)\n",
      "  (bilstm1): LSTM(200 -> 300, TNC, num_layers=3, dropout=0.5, bidirectional)\n",
      "  (bilstm2): LSTM(2400 -> 300, TNC, dropout=0.5, bidirectional)\n",
      "  (align_att): AlignAttention(\n",
      "  \n",
      "  )\n",
      "  (output_layer): HybridSequential(\n",
      "    (0): Dense(2400 -> 1024, Activation(tanh))\n",
      "    (1): Dropout(p = 0.5, axes=())\n",
      "    (2): Dense(1024 -> 1024, Activation(tanh))\n",
      "    (3): Dropout(p = 0.5, axes=())\n",
      "    (4): Dense(1024 -> 2, linear)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "test_model = True\n",
    "vocab_len = len(vocab)\n",
    "emsize = 200    # word embedding size\n",
    "nhidden = 300   # lstm hidden_dim\n",
    "nlayers = 3     # lstm layers\n",
    "\n",
    "# final fc layer's number of hidden units and predicted number of classes\n",
    "nfc = 1024\n",
    "nclass = 2\n",
    "\n",
    "drop_prob = 0.5\n",
    "\n",
    "ctx = mx.gpu(0)\n",
    "\n",
    "model = EnhancedSeqInfer(vocab_len, emsize, nhidden, nlayers, nfc, nclass, drop_prob)\n",
    "\n",
    "if test_model:\n",
    "    model.load_parameters('model/esim-0.79.params', ctx=ctx)\n",
    "else:\n",
    "    model.initialize(init=init.Xavier(), ctx=ctx)\n",
    "# Attach a pre-trained glove word vector to the embedding layer\n",
    "model.embedding_layer.weight.set_data(vocab.embedding.idx_to_vec)\n",
    "# fixed the embedding layer\n",
    "model.embedding_layer.collect_params().setattr('grad_req', 'null')\n",
    "\n",
    "print(model)\n",
    "\n",
    "train_curve, valid_curve = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(x_left, x_right, y, model, loss, class_weight):\n",
    "    pred, att = model(x_left, x_right)\n",
    "    y = nd.array(y.asnumpy().astype('int32')).as_in_context(ctx)\n",
    "    if loss_name == 'sce':\n",
    "        l = loss(pred, y)\n",
    "    elif loss_name == 'wsce':\n",
    "        l = loss(pred, y, class_weight, class_weight.shape[0])\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "    return pred, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(data_iter, model, loss, trainer, ctx, is_train, epoch, \\\n",
    "              clip=None, class_weight=None, loss_name='sce'):\n",
    "\n",
    "    loss_val = 0.\n",
    "    total_pred = []\n",
    "    total_true = []\n",
    "    n_batch = 0\n",
    "\n",
    "    for batch_x_left, batch_x_right, batch_y in data_iter:\n",
    "        batch_x_left = batch_x_left.as_in_context(ctx)\n",
    "        batch_x_right = batch_x_right.as_in_context(ctx)\n",
    "        batch_y = batch_y.as_in_context(ctx)\n",
    "\n",
    "        if is_train:\n",
    "            with autograd.record():\n",
    "                batch_pred, l = calculate_loss(batch_x_left, batch_x_right, \\\n",
    "                                               batch_y, model, loss, class_weight)\n",
    "\n",
    "            # backward calculate\n",
    "            l.backward()\n",
    "\n",
    "            # clip gradient\n",
    "            clip_params = [p.data() for p in model.collect_params().values()]\n",
    "            if clip is not None:\n",
    "                norm = nd.array([0.0], ctx)\n",
    "                for param in clip_params:\n",
    "                    if param.grad is not None:\n",
    "                        norm += (param.grad ** 2).sum()\n",
    "                norm = norm.sqrt().asscalar()\n",
    "                if norm > clip:\n",
    "                    for param in clip_params:\n",
    "                        if param.grad is not None:\n",
    "                            param.grad[:] *= clip / norm\n",
    "\n",
    "            # update parmas\n",
    "            trainer.step(batch_x_left.shape[0])\n",
    "\n",
    "        else:\n",
    "            batch_pred, l = calculate_loss(batch_x_left, batch_x_right, \\\n",
    "                                           batch_y, model, loss, class_weight)\n",
    "\n",
    "        # keep result for metric\n",
    "        batch_pred = nd.argmax(nd.softmax(batch_pred, axis=1), axis=1).asnumpy()\n",
    "        batch_true = np.reshape(batch_y.asnumpy(), (-1, ))\n",
    "        total_pred.extend(batch_pred.tolist())\n",
    "        total_true.extend(batch_true.tolist())\n",
    "        \n",
    "        batch_loss = l.mean().asscalar()\n",
    "\n",
    "        n_batch += 1\n",
    "        loss_val += batch_loss\n",
    "\n",
    "        # check the result of traing phase\n",
    "        if is_train and n_batch % 400 == 0:\n",
    "            print('epoch %d, batch %d, batch_train_loss %.4f, batch_train_acc %.3f' %\n",
    "                  (epoch, n_batch, batch_loss, accuracy_score(batch_true, batch_pred)))\n",
    "\n",
    "    # metric\n",
    "    F1 = f1_score(np.array(total_true), np.array(total_pred), average='binary')\n",
    "    acc = accuracy_score(np.array(total_true), np.array(total_pred))\n",
    "    loss_val /= n_batch\n",
    "\n",
    "    if is_train:\n",
    "        print('epoch %d, learning_rate %.5f \\n\\t train_loss %.4f, acc_train %.3f, F1_train %.3f, ' %\n",
    "              (epoch, trainer.learning_rate, loss_val, acc, F1))\n",
    "        train_curve.append((acc, F1))\n",
    "        # declay lr\n",
    "        if epoch % 2 == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * 0.9)\n",
    "    else:\n",
    "        print('\\t valid_loss %.4f, acc_valid %.3f, F1_valid %.3f, ' % (loss_val, acc, F1))\n",
    "        valid_curve.append((acc, F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid(data_iter_train, data_iter_valid, model, loss, trainer, \\\n",
    "                ctx, nepochs, clip=None, class_weight=None, loss_name='sce'):\n",
    "\n",
    "    for epoch in range(1, nepochs+1):\n",
    "        start = time.time()\n",
    "        # train\n",
    "        is_train = True\n",
    "        one_epoch(data_iter_train, model, loss, trainer, ctx, is_train,\n",
    "                  epoch, clip, class_weight, loss_name)\n",
    "\n",
    "        # valid\n",
    "        is_train = False\n",
    "        one_epoch(data_iter_valid, model, loss, trainer, ctx, is_train,\n",
    "                  epoch, clip, class_weight, loss_name)\n",
    "        end = time.time()\n",
    "        print('time %.2f sec' % (end-start))\n",
    "        print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Now that we are training the model, we use WeightedSoftmaxCE to alleviate the problem of data category imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = None\n",
    "loss_name = 'sce'\n",
    "optim = 'adam'\n",
    "lr = 0.001\n",
    "clip = .5\n",
    "nepochs = 10\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), optim, {'learning_rate': lr})\n",
    "\n",
    "if loss_name == 'sce':\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "elif loss_name == 'wsce':\n",
    "    loss = WeightedSoftmaxCE()\n",
    "    # the value of class_weight is obtained by counting data in advance. It can be seen as a hyperparameter.\n",
    "    class_weight = nd.array([1., 3.], ctx=ctx)\n",
    "else:\n",
    "    print('loss function {} is not implemented!'.format(loss_name))\n",
    "    raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 400, batch_train_loss 0.7526, batch_train_acc 0.641\n",
      "epoch 1, batch 800, batch_train_loss 0.7259, batch_train_acc 0.641\n",
      "epoch 1, batch 1200, batch_train_loss 0.4684, batch_train_acc 0.781\n",
      "epoch 1, batch 1600, batch_train_loss 0.4696, batch_train_acc 0.870\n",
      "epoch 1, batch 2000, batch_train_loss 0.4977, batch_train_acc 0.797\n",
      "epoch 1, batch 2400, batch_train_loss 0.4754, batch_train_acc 0.812\n",
      "epoch 1, batch 2800, batch_train_loss 0.3645, batch_train_acc 0.875\n",
      "epoch 1, batch 3200, batch_train_loss 0.5161, batch_train_acc 0.781\n",
      "epoch 1, batch 3600, batch_train_loss 0.5540, batch_train_acc 0.750\n",
      "epoch 1, batch 4000, batch_train_loss 0.4992, batch_train_acc 0.766\n",
      "epoch 1, batch 4400, batch_train_loss 0.7088, batch_train_acc 0.625\n",
      "epoch 1, batch 4800, batch_train_loss 0.5714, batch_train_acc 0.672\n",
      "epoch 1, batch 5200, batch_train_loss 0.5199, batch_train_acc 0.703\n",
      "epoch 1, batch 5600, batch_train_loss 0.5471, batch_train_acc 0.734\n",
      "epoch 1, batch 6000, batch_train_loss 0.3560, batch_train_acc 0.883\n",
      "epoch 1, batch 6400, batch_train_loss 0.3609, batch_train_acc 0.857\n",
      "epoch 1, batch 6800, batch_train_loss 0.6237, batch_train_acc 0.672\n",
      "epoch 1, batch 7200, batch_train_loss 0.5938, batch_train_acc 0.672\n",
      "epoch 1, batch 7600, batch_train_loss 0.1817, batch_train_acc 0.920\n",
      "epoch 1, learning_rate 0.00100 \n",
      "\t train_loss 0.5132, acc_train 0.770, F1_train 0.402, \n",
      "\t valid_loss 0.4811, acc_valid 0.790, F1_valid 0.446, \n",
      "time 8226.12 sec\n",
      "****************************************************************************************************\n",
      "epoch 2, batch 400, batch_train_loss 0.4511, batch_train_acc 0.797\n",
      "epoch 2, batch 800, batch_train_loss 0.2964, batch_train_acc 0.896\n",
      "epoch 2, batch 1200, batch_train_loss 0.4690, batch_train_acc 0.797\n",
      "epoch 2, batch 1600, batch_train_loss 0.3525, batch_train_acc 0.844\n",
      "epoch 2, batch 2000, batch_train_loss 0.5158, batch_train_acc 0.703\n",
      "epoch 2, batch 2400, batch_train_loss 0.4190, batch_train_acc 0.831\n",
      "epoch 2, batch 2800, batch_train_loss 0.6197, batch_train_acc 0.656\n",
      "epoch 2, batch 3200, batch_train_loss 0.6784, batch_train_acc 0.609\n",
      "epoch 2, batch 3600, batch_train_loss 0.5291, batch_train_acc 0.750\n",
      "epoch 2, batch 4000, batch_train_loss 0.4475, batch_train_acc 0.781\n",
      "epoch 2, batch 4400, batch_train_loss 0.6738, batch_train_acc 0.688\n",
      "epoch 2, batch 4800, batch_train_loss 0.3731, batch_train_acc 0.828\n",
      "epoch 2, batch 5200, batch_train_loss 0.4714, batch_train_acc 0.828\n",
      "epoch 2, batch 5600, batch_train_loss 0.3271, batch_train_acc 0.883\n"
     ]
    }
   ],
   "source": [
    "# train and valid\n",
    "train_valid(train_dataloader, valid_dataloader, model, loss, \\\n",
    "            trainer, ctx, nepochs, clip=clip, class_weight=class_weight, loss_name=loss_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "Now we will randomly input a movie plot summary and its review to see if the model decide the review is a spoiler or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_parameters('model/esim-{:.4}.params'.format(str(valid_curve[-1][0])))\n",
    "with open('acc_record', 'w') as f:\n",
    "    f.write(str(train_curve)+'\\n')\n",
    "    f.write(str(valid_curve))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_parameters('model/esim-0.78.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = 'john is a former police, his daughter, sarah was killed in a car accident. with his investigation going deeper, he found that sarah\\'s boyfriend, jack, was the one to blame.'\n",
    "left = 'I liked sarah and jack in this movie'\n",
    "left_token = vocab[tokenizer(left)]\n",
    "right_token = vocab[tokenizer(right)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 1.2295518 -1.0533029]]\n",
       "<NDArray 1x2 @gpu(0)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_input = nd.array(left_token, ctx=ctx).reshape(1,-1)\n",
    "right_input = nd.array(right_token, ctx=ctx).reshape(1,-1)\n",
    "pred, att = model(left_input, right_input)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f50806d4630>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2736x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "vis_att = np.squeeze(att.asnumpy())\n",
    "plt.figure(figsize=vis_att.T.shape)\n",
    "sns.heatmap(vis_att, xticklabels=tokenizer(right), yticklabels=tokenizer(left), center=0.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
