{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Self-attentive Sentence Embedding\n",
    "\n",
    "The code is based on [gluon-nlp tutorial](https://gluon-nlp.mxnet.io/examples/sentence_embedding/self_attentive_sentence_embedding.html).\n",
    "\n",
    "## Import Related Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import multiprocessing as mp\n",
    "import gluonnlp as nlp\n",
    "\n",
    "from mxnet import gluon, nd, init\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet import autograd, gluon, nd\n",
    "from d2l import try_gpu\n",
    "import pandas as pd\n",
    "\n",
    "# iUse sklearn's metric function to evaluate the results of the experiment\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# fixed random number seed\n",
    "np.random.seed(9102)\n",
    "mx.random.seed(9102)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline\n",
    "\n",
    "### Load Dataset\n",
    "\n",
    "See [dataloader](data_loader.ipynb) for how the training samples are obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 1000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = 'data/imdb/'\n",
    "file_name = 'train.csv'\n",
    "file_path = data_folder + file_name\n",
    "\n",
    "## load json data.\n",
    "nrows = 5000\n",
    "data = pd.read_csv(file_path, nrows=nrows)\n",
    "\n",
    "# create a list of review a label paris.\n",
    "dataset = [[left, right, int(label)] for left, right, label in \\\n",
    "           zip(data['review_text'], data['plot_summary'], data['is_spoiler'])]\n",
    "\n",
    "# randomly divide one percent from the training set as a verification set.\n",
    "train_dataset, valid_dataset = nlp.data.train_valid_split(dataset, 0.2)\n",
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "\n",
    "The purpose of the following code is to process the raw data so that the processed data can be used for model training and prediction. We will use the `SpacyTokenizer` to split the document into tokens, `ClipSequence` to crop the comments to the specified length, and build a vocabulary based on the word frequency of the training data. Then we attach the [Glove](https://nlp.stanford.edu/pubs/glove.pdf)[2]  pre-trained word vector to the vocabulary and convert each token into the corresponding word index in the vocabulary.\n",
    "Finally get the standardized training data set and verification data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Tokenizing Time=2.19s, #Sentences=4000\n",
      "Done! Tokenizing Time=0.88s, #Sentences=1000\n"
     ]
    }
   ],
   "source": [
    "# tokenizer takes as input a string and outputs a list of tokens.\n",
    "tokenizer = nlp.data.SpacyTokenizer('en')\n",
    "\n",
    "# length_clip takes as input a list and outputs a list with maximum length 300.\n",
    "length_clip_review = nlp.data.ClipSequence(300)\n",
    "length_clip_plot = nlp.data.ClipSequence(100)\n",
    "\n",
    "def preprocess(x):\n",
    "\n",
    "    # now the first element in tuple is review, second plot and third label\n",
    "    try:\n",
    "        left, right, label = x[0], x[1], int(x[2])\n",
    "        assert(type(left)==type('str') and type(right)==type('str'))\n",
    "        assert(label==0 or label==1)\n",
    "    except:\n",
    "        print(left, right)\n",
    "        left, right=str(left), str(right)\n",
    "    # clip the length of review words\n",
    "    left, right = length_clip_review(tokenizer(left.lower())), \\\n",
    "                  length_clip_plot(tokenizer(right.lower()))\n",
    "    return left, right, label\n",
    "\n",
    "def get_length(x):\n",
    "    return float(len(x[0]))\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    start = time.time()\n",
    "\n",
    "    with mp.Pool() as pool:\n",
    "        # Each sample is processed in an asynchronous manner.\n",
    "        dataset = gluon.data.ArrayDataset(pool.map(preprocess, dataset))\n",
    "        lengths = gluon.data.ArrayDataset(pool.map(get_length, dataset))\n",
    "    end = time.time()\n",
    "\n",
    "    print('Done! Tokenizing Time={:.2f}s, #Sentences={}'.format(end - start, len(dataset)))\n",
    "    return dataset, lengths\n",
    "\n",
    "# Preprocess the dataset\n",
    "train_dataset, train_data_lengths = preprocess_dataset(train_dataset)\n",
    "valid_dataset, valid_data_lengths = preprocess_dataset(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=20004, unk=\"<unk>\", reserved=\"['<pad>', '<bos>', '<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "# create vocab\n",
    "train_seqs = [sample[0]+sample[1] for sample in train_dataset]\n",
    "counter = nlp.data.count_tokens(list(itertools.chain.from_iterable(train_seqs)))\n",
    "\n",
    "vocab = nlp.Vocab(counter, max_size=20000)\n",
    "\n",
    "# load pre-trained embedding, Glove\n",
    "embedding_weights = nlp.embedding.GloVe(source='glove.twitter.27B.200d')\n",
    "vocab.set_embedding(embedding_weights)\n",
    "print(vocab)\n",
    "\n",
    "# NOTE: to use the same encoder, we need to ensure that two inputs are of the same length\n",
    "# this is achieved by manual padding\n",
    "def token_to_idx(x):\n",
    "    return vocab[x[0]], vocab[x[1]], x[2]\n",
    "\n",
    "# A token index or a list of token indices is returned according to the vocabulary.\n",
    "with mp.Pool() as pool:\n",
    "    train_dataset = pool.map(token_to_idx, train_dataset)\n",
    "    valid_dataset = pool.map(token_to_idx, valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14436, \n",
       " [ 7.3501e-01  2.3659e-01  8.1485e-02  6.9116e-02 -8.2966e-02  5.1736e-01\n",
       "  -2.2106e-01  1.2918e-01 -1.4550e-01  1.8944e-01  3.4552e-01 -2.7266e-01\n",
       "   2.5191e-01  4.1132e-02 -2.9738e-01 -1.9861e-01  2.3085e-01  2.0603e-01\n",
       "  -4.2740e-02  2.6072e-01 -6.5018e-01  5.0332e-02  1.6095e-01 -5.4888e-01\n",
       "  -2.7124e-01  1.1846e-01 -5.7249e-02 -3.1976e-01 -1.1128e-01  1.8404e-01\n",
       "  -2.0640e-01  4.7762e-01  3.5954e-02 -1.4585e-02 -6.9505e-02  6.1174e-01\n",
       "   4.4875e-01 -5.2554e-01 -1.5218e-01  3.7220e-01  3.6481e-01 -4.6937e-01\n",
       "  -9.1761e-02 -6.8574e-01 -5.4097e-01  8.5439e-01 -3.5236e-01 -2.0113e-01\n",
       "  -2.0686e-01 -1.0377e+00  4.0393e-01 -7.6687e-01 -2.3442e-01  2.8207e-01\n",
       "  -8.8382e-01 -5.5162e-03  6.4569e-01 -6.6990e-02  6.1193e-01  7.7833e-02\n",
       "   1.2878e-01  1.1748e-01 -4.4662e-01 -1.5219e-01 -9.0765e-01  6.0395e-01\n",
       "   1.4690e-01  2.9322e-01 -2.7783e-01 -5.7250e-01 -3.1507e-01  2.0820e-01\n",
       "  -1.5681e-01  1.1861e+00  5.2476e-01  3.4866e-01 -4.0212e-01 -3.9851e-01\n",
       "   2.9129e-01 -2.7880e-01  2.8829e-01  1.9564e-01 -2.7899e-01  3.6847e-01\n",
       "   1.3826e-01 -6.9025e-01 -4.1249e-01 -7.1170e-01  2.8858e-01  2.7638e-01\n",
       "   3.6034e-01  4.2460e-01 -1.5351e-01  1.9459e-01  7.9911e-01 -4.0513e-02\n",
       "   2.9826e-01 -8.3249e-02  6.8677e-01  2.3503e-01  5.2818e-01 -3.4757e-01\n",
       "  -2.6098e-01  2.9453e-01 -5.3556e-01 -7.7834e-02 -2.9957e-01 -9.3032e-02\n",
       "   6.4985e-01  7.2779e-01 -4.7749e-01  2.4823e-01 -2.9187e-01  7.2446e-01\n",
       "   1.7321e-01  4.1393e-01  1.1927e-01 -4.2901e-01 -2.3067e-01  1.8209e-01\n",
       "   1.0044e+00 -2.7857e-02  6.2388e-03  1.9663e-01  5.7858e-02 -2.0618e-01\n",
       "   2.8117e-01 -6.6775e-01 -2.1352e-02  3.2140e-01  2.3975e-01  5.9901e-01\n",
       "   2.2148e-02 -6.5890e-01 -2.6506e-01 -1.6623e-01  1.1656e-01 -2.0030e-01\n",
       "  -3.6758e-01 -6.8177e-01  7.0396e-01  1.0068e-01  2.4387e-02  5.7095e-01\n",
       "   6.1634e-01  7.8464e-02 -4.4905e-01  2.9945e-01 -4.3499e-02  3.8068e-01\n",
       "   7.3008e-01  3.3006e-01 -7.2527e-01 -3.6724e-01 -5.7050e-01  2.7842e-01\n",
       "   5.6509e-01 -6.1163e-01 -6.0670e-01  7.4337e-03 -1.9073e-01  8.2312e-05\n",
       "  -3.1425e-01  5.3427e-01  5.6350e-01  8.3806e-01 -3.6500e-01  1.1097e-01\n",
       "   3.4283e-01 -9.2376e-02  1.5929e-02  3.7449e-01  2.3812e-01 -1.5049e-01\n",
       "   2.5985e-01 -4.8193e-01 -9.5252e-01 -5.2063e-01 -4.1430e-01  6.4254e-01\n",
       "  -9.4603e-01  1.8581e-03 -3.6450e-01 -9.3968e-02 -2.2652e-04  1.1164e+00\n",
       "  -2.2338e-01 -7.4049e-01  1.9683e-01  2.5416e-01  4.6923e-01 -3.2709e-01\n",
       "   2.3938e-01  1.7447e-01  2.8088e-02 -4.9827e-01  9.5068e-01  1.3318e-01\n",
       "   6.8061e-02  4.1904e-03]\n",
       " <NDArray 200 @cpu(0)>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = vocab.embedding.token_to_idx['xmen']\n",
    "idx, vocab.embedding.idx_to_vec[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketing and DataLoader\n",
    "Since each sentence may have a different length, we need to use `Pad` to fill the sentences in a minibatch to equal lengths so that the data can be quickly tensored on the GPU. At the same time, we need to use `Stack` to stack the category tags of a batch of data. For convenience, we use `Tuple` to combine `Pad` and `Stack`.\n",
    "\n",
    "In order to make the length of the sentence pad in each minibatch as small as possible, we should make the sentences with similar lengths in a batch as much as possible. In light of this, we consider constructing a sampler using `FixedBucketSampler`, which defines how the samples in a dataset will be iterated in a more economic way.\n",
    "\n",
    "Finally, we use `DataLoader` to build a data loader for the training. dataset and validation dataset. The training dataset requires FixedBucketSampler, but the validation dataset doesn't require the sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=4000, batch_num=64\n",
      "  key=[48, 76, 104, 132, 160, 188, 216, 244, 272, 300]\n",
      "  cnt=[23, 110, 159, 255, 490, 480, 373, 248, 235, 1627]\n",
      "  batch_size=[200, 126, 92, 72, 64, 64, 64, 64, 64, 64]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "bucket_num = 10\n",
    "bucket_ratio = 0.5\n",
    "\n",
    "\n",
    "def get_dataloader():\n",
    "    # Construct the DataLoader Pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(axis=0), \\\n",
    "                                          nlp.data.batchify.Pad(axis=0),\n",
    "                                          nlp.data.batchify.Stack())\n",
    "\n",
    "    # n this example, we use a FixedBucketSampler,\n",
    "    # which assigns each data sample to a fixed bucket based on its length.\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_data_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "\n",
    "    # train_dataloader\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn)\n",
    "    # valid_dataloader\n",
    "    valid_dataloader = gluon.data.DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn)\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "train_dataloader, valid_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# experiement one the two dataloaders\n",
    "# for left, right, label in train_dataloader:\n",
    "    # print(left.shape, right.shape, label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Structure\n",
    "\n",
    "In the original paper, the representation of the sentence is: firstly, the sentence is disassembled into a list corresponding to the word, then the words are unrolled in order, and the word vector of each word is calculated as the input of each step of the [bidirectional LSTM neural network layer](https://www.bioinf.jku.at/publications/older/2604.pdf)[3]. Taking the output of each step of the bidirectional LSTM network layer, a matrix H is obtained. Suppose the hidden_dim of the bidirectional LSTM is `u`, the word length of the sentence is `n`, then the dimension of the last H is `n-by-2u`.  For example, the sentence \"this movie is amazing\" would be represented as:\n",
    "![](images/Bi-LSTM-Rep.png)\n",
    "\n",
    "Attention is like when we are looking at things, we always give different importance to things in the scope of the perspective. For example, when we are communicating with people, our eyes will always pay more attention to the face of the communicator, not to the edge of other perspectives. So when we want to express the sentence, we can pay different attention to the output H of the bidirectional LSTM layer.\n",
    "![](images/attention-nlp.png)\n",
    "$$\n",
    "A = Softmax(W_{s2}tanh(W_{s1}H^T))\n",
    "$$\n",
    "\n",
    "Here, W<sub>s1</sub> is a weight matrix with the shape: d<sub>a</sub>-by-2u, where d<sub>a</sub> is a hyperparameter.\n",
    "W<sub>s2</sub> is a weight matrix with the shape: r-by-d<sub>a</sub>, where r is the number of different attentions you want to use.\n",
    "\n",
    "When the attention matrix A and the output H of the LSTM are obtained, the final representation is obtained by M=AH.\n",
    "\n",
    "\n",
    "\n",
    "We can first customize a layer of attention, specify the number of hidden nodes att_unit and the number of attention channels att_hops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttention(\n",
      "  (ut_dense): Dense(None -> 20, Activation(tanh))\n",
      "  (et_dense): Dense(None -> 5, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# custom attention layer\n",
    "# in this class, we want to implement the operation:\n",
    "# softmax(W_2 * tanh(W_1 * H))\n",
    "# where H is the word embedding of the whole sentence, of shape (num_of_word, embed_size)\n",
    "class SelfAttention(nn.HybridBlock):\n",
    "    def __init__(self, att_unit, att_hops, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # this layer is tanh(w_1 * H), the att_unit corresponds to d_a in the essay\n",
    "            self.ut_dense = nn.Dense(att_unit, activation='tanh', flatten=False)\n",
    "            # this layer implements the multiple hops\n",
    "            self.et_dense = nn.Dense(att_hops, activation=None, flatten=False)\n",
    "\n",
    "    def hybrid_forward(self, F, x): # F is the backend which implements the tensor operation\n",
    "        # x shape: [batch_size, seq_len, embedding_width]\n",
    "        # ut shape: [batch_size, seq_len, att_unit]\n",
    "        ut = self.ut_dense(x) # batch_size * seq_len [* embed_size * embed_size *] att_unit\n",
    "        # et shape: [batch_size, seq_len, att_hops]\n",
    "        et = self.et_dense(ut)# batch_size * seq_len [* att_unit * att_unit *] att_hops\n",
    "\n",
    "        # att shape: [batch_size,  att_hops, seq_len]\n",
    "        # softmax is performed along the seq_len dimension\n",
    "        att = F.softmax(F.transpose(et, axes=(0, 2, 1)), axis=-1)\n",
    "        # output shape [batch_size, att_hops, embedding_width]\n",
    "        output = F.batch_dot(att, x)\n",
    "        # output is the weighted matrix representation of the matrix\n",
    "        # att is the weighted vector we use as attention\n",
    "        return output, att\n",
    "    \n",
    "# d_a = 20, hops = 5\n",
    "print(SelfAttention(20, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the number of samples for labels are very unbalanced, applying different weights on different labels may improve the performance of the model. This can be seen as a method to correct [covariance shift](http://d2l.ai/chapter_multilayer-perceptrons/environment.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSoftmaxCE(nn.HybridBlock):\n",
    "    def __init__(self, sparse_label=True, from_logits=False,  **kwargs):\n",
    "        super(WeightedSoftmaxCE, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.sparse_label = sparse_label\n",
    "            self.from_logits = from_logits\n",
    "\n",
    "    def hybrid_forward(self, F, pred, label, class_weight, depth=None):\n",
    "        if self.sparse_label:\n",
    "            label = F.reshape(label, shape=(-1, ))\n",
    "            label = F.one_hot(label, depth)\n",
    "        if not self.from_logits:\n",
    "            pred = F.log_softmax(pred, -1)\n",
    "\n",
    "        weight_label = F.broadcast_mul(label, class_weight)\n",
    "        loss = -F.sum(pred * weight_label, axis=-1)\n",
    "\n",
    "        # return F.mean(loss, axis=0, exclude=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentiveBiLSTM(nn.HybridBlock):\n",
    "    def __init__(self, vocab_len, embsize, nhidden, nlayers, natt_unit, natt_hops, \\\n",
    "                 nfc, nclass, # these two params are not used currrently\n",
    "                 drop_prob, pool_way, prune_p=None, prune_q=None, **kwargs):\n",
    "        super(SelfAttentiveBiLSTM, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # now we switch back to shared layers\n",
    "            self.embedding_layer = nn.Embedding(vocab_len, embsize)\n",
    "            \n",
    "            self.bilstm = rnn.LSTM(nhidden, num_layers=nlayers, dropout=drop_prob, \\\n",
    "                                        bidirectional=True)\n",
    "            \n",
    "            self.att_encoder = SelfAttention(natt_unit, natt_hops)\n",
    "            # this layer is used to output the final class\n",
    "            self.output_layer = nn.Dense(nclass, activation='tanh')\n",
    "            \n",
    "            self.dense_p, self.dense_q = None, None\n",
    "            if all([prune_p, prune_q]):\n",
    "                self.dense_p = nn.Dense(prune_p, activation='tanh', flatten=False)\n",
    "                self.dense_q = nn.Dense(prune_q, activation='tanh', flatten=False)\n",
    "\n",
    "            self.drop_prob = drop_prob\n",
    "            self.pool_way = pool_way\n",
    "\n",
    "    def hybrid_forward(self, F, inp_left, inp_right):\n",
    "        # inp is a list containing left_text and right_text\n",
    "        # their size: [batch, token_idx]\n",
    "        # inp_embed_left/right size: [batch, seq_len, embed_size]\n",
    "        inp_embed_left = self.embedding_layer(inp_left)\n",
    "        inp_embed_right = self.embedding_layer(inp_right)\n",
    "        # rnn requires the first dimension to be the time steps\n",
    "        h_output_left = self.bilstm(F.transpose(inp_embed_left, axes=(1, 0, 2)))\n",
    "        h_output_right = self.bilstm(F.transpose(inp_embed_right, axes=(1, 0, 2)))\n",
    "        # att_output: [batch, att_hops, emsize]\n",
    "        output_left, att_left = self.att_encoder(F.transpose(h_output_left, \\\n",
    "                                                             axes=(1, 0, 2)))\n",
    "        output_right, att_right = self.att_encoder(F.transpose(h_output_right, \\\n",
    "                                                               axes=(1, 0, 2)))\n",
    "        '''\n",
    "        FIXME: now this code will only work with flatten\n",
    "        '''\n",
    "        dense_input_left, dense_input_right = None, None\n",
    "        if self.pool_way == 'flatten':\n",
    "            dense_input_left = F.Dropout(F.flatten(output_left), self.drop_prob)\n",
    "            dense_input_right = F.Dropout(F.flatten(output_right), self.drop_prob)\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "        '''\n",
    "        elif self.pool_way == 'mean':\n",
    "            dense_input = F.Dropout(F.mean(att_output, axis=1), self.drop_prob)\n",
    "        elif self.pool_way == 'prune' and all([self.dense_p, self.dense_q]):\n",
    "            # p_section: [batch, att_hops, prune_p]\n",
    "            p_section = self.dense_p(att_output)\n",
    "            # q_section: [batch, emsize, prune_q]\n",
    "            q_section = self.dense_q(F.transpose(att_output, axes=(0, 2, 1)))\n",
    "            dense_input = F.Dropout(F.concat(F.flatten(p_section), F.flatten(q_section), dim=-1), self.drop_prob)\n",
    "        '''\n",
    "        dense_input = dense_input_left - dense_input_right\n",
    "        output = self.output_layer(dense_input)\n",
    "\n",
    "        return output, att_left, att_right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure parameters and build models\n",
    "\n",
    "The resulting `M` is a matrix, and the way to classify this matrix is `flatten`, `mean` or `prune`. Prune is a way of trimming parameters proposed in the original paper and has been implemented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttentiveBiLSTM(\n",
      "  (embedding_layer): Embedding(20004 -> 200, float32)\n",
      "  (bilstm): LSTM(None -> 300, TNC, num_layers=4, bidirectional)\n",
      "  (att_encoder): SelfAttention(\n",
      "    (ut_dense): Dense(None -> 300, Activation(tanh))\n",
      "    (et_dense): Dense(None -> 20, linear)\n",
      "  )\n",
      "  (output_layer): Dense(None -> 2, Activation(tanh))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_len = len(vocab)\n",
    "emsize = 200   # word embedding size\n",
    "nhidden = 300    # lstm hidden_dim\n",
    "nlayers = 4     # lstm layers\n",
    "natt_unit = 300     # the hidden_units of attention layer\n",
    "natt_hops = 20    # the channels of attention\n",
    "\n",
    "# these two variables are not used, preserve for now\n",
    "nfc = 512\n",
    "nclass = 2\n",
    "\n",
    "drop_prob = 0\n",
    "pool_way = 'flatten'    # # The way to handle M\n",
    "prune_p = None\n",
    "prune_q = None\n",
    "\n",
    "ctx = try_gpu()\n",
    "\n",
    "model = SelfAttentiveBiLSTM(vocab_len, emsize, nhidden, nlayers,\n",
    "                            natt_unit, natt_hops, nfc, nclass,\n",
    "                            drop_prob, pool_way, prune_p, prune_q)\n",
    "\n",
    "model.initialize(init=init.Xavier(), ctx=ctx)\n",
    "model.hybridize()\n",
    "\n",
    "# Attach a pre-trained glove word vector to the embedding layer\n",
    "model.embedding_layer.weight.set_data(vocab.embedding.idx_to_vec)\n",
    "# fixed the embedding layer\n",
    "model.embedding_layer.collect_params().setattr('grad_req', 'null')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using r attention can improve the representation of sentences with different semantics, but if the value of each line in the attention matrix A (r-byn) is very close, that is, there is no difference between several attentions. Subsequently, in M = AH, the resulting M will contain a lot of redundant information.\n",
    "So in order to solve this problem, we should try to force A to ensure that the value of each line has obvious differences, that is, try to satisfy the diversity of attention. Therefore, a penalty can be used to achieve this goal.\n",
    "$$ P = ||(AA^T-I)||_F^2 $$\n",
    "\n",
    "\n",
    "It can be seen from the above formula that if the value of each row of A is more similar, the result of P will be larger, and the value of A is less similar for each row, and P is smaller. This means that when the r-focused diversity of A is larger, the smaller P is. So by including this penalty item with the Loss of the model, you can try to ensure the diversity of A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(x_left, x_right, y, model, loss, class_weight, penal_coeff):\n",
    "    pred, att_left, att_right = model(x_left, x_right)\n",
    "    y = nd.array(y.asnumpy().astype('int32')).as_in_context(ctx)\n",
    "    if loss_name in ['sce', 'l1', 'l2']:\n",
    "        l = loss(pred, y)\n",
    "    elif loss_name == 'wsce':\n",
    "        l = loss(pred, y, class_weight, class_weight.shape[0])\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "    # penalty, now we have two att's\n",
    "    diversity_penalty_left = nd.batch_dot(att_left, nd.transpose(att_left, axes=(0, 2, 1))) - \\\n",
    "                             nd.eye(att_left.shape[1], ctx=att_left.context)\n",
    "    diversity_penalty_right = nd.batch_dot(att_right, nd.transpose(att_right, axes=(0, 2, 1))) - \\\n",
    "                              nd.eye(att_right.shape[1], ctx=att_right.context)\n",
    "    l = l + penal_coeff * (diversity_penalty_left.norm(axis=(1, 2)) + \\\n",
    "                           diversity_penalty_right.norm(axis=(1, 2)))\n",
    "\n",
    "    return pred, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(data_iter, model, loss, trainer, ctx, is_train, epoch,\n",
    "              penal_coeff=0.0, clip=None, class_weight=None, loss_name='sce'):\n",
    "\n",
    "    loss_val = 0.\n",
    "    total_pred = []\n",
    "    total_true = []\n",
    "    n_batch = 0\n",
    "\n",
    "    for batch_x_left, batch_x_right, batch_y in data_iter:\n",
    "        batch_x_left = batch_x_left.as_in_context(ctx)\n",
    "        batch_x_right = batch_x_right.as_in_context(ctx)\n",
    "        batch_y = batch_y.as_in_context(ctx)\n",
    "\n",
    "        if is_train:\n",
    "            with autograd.record():\n",
    "                batch_pred, l = calculate_loss(batch_x_left, batch_x_right, \\\n",
    "                                               batch_y, model, loss, class_weight, \\\n",
    "                                               penal_coeff)\n",
    "\n",
    "            # backward calculate\n",
    "            l.backward()\n",
    "\n",
    "            # clip gradient\n",
    "            clip_params = [p.data() for p in model.collect_params().values()]\n",
    "            if clip is not None:\n",
    "                norm = nd.array([0.0], ctx)\n",
    "                for param in clip_params:\n",
    "                    if param.grad is not None:\n",
    "                        norm += (param.grad ** 2).sum()\n",
    "                norm = norm.sqrt().asscalar()\n",
    "                if norm > clip:\n",
    "                    for param in clip_params:\n",
    "                        if param.grad is not None:\n",
    "                            param.grad[:] *= clip / norm\n",
    "\n",
    "            # update parmas\n",
    "            trainer.step(batch_x_left.shape[0])\n",
    "\n",
    "        else:\n",
    "            batch_pred, l = calculate_loss(batch_x_left, batch_x_right, \\\n",
    "                                           batch_y, model, loss, class_weight, \\\n",
    "                                           penal_coeff)\n",
    "\n",
    "        # keep result for metric\n",
    "        batch_pred = nd.argmax(nd.softmax(batch_pred, axis=1), axis=1).asnumpy()\n",
    "        batch_true = np.reshape(batch_y.asnumpy(), (-1, ))\n",
    "        total_pred.extend(batch_pred.tolist())\n",
    "        total_true.extend(batch_true.tolist())\n",
    "        \n",
    "        batch_loss = l.mean().asscalar()\n",
    "\n",
    "        n_batch += 1\n",
    "        loss_val += batch_loss\n",
    "\n",
    "        # check the result of traing phase\n",
    "        if is_train and n_batch % 400 == 0:\n",
    "            print('epoch %d, batch %d, batch_train_loss %.4f, batch_train_acc %.3f' %\n",
    "                  (epoch, n_batch, batch_loss, accuracy_score(batch_true, batch_pred)))\n",
    "\n",
    "    # metric\n",
    "    F1 = f1_score(np.array(total_true), np.array(total_pred), average='binary')\n",
    "    acc = accuracy_score(np.array(total_true), np.array(total_pred))\n",
    "    loss_val /= n_batch\n",
    "\n",
    "    if is_train:\n",
    "        print('epoch %d, learning_rate %.5f \\n\\t train_loss %.4f, acc_train %.3f, F1_train %.3f, ' %\n",
    "              (epoch, trainer.learning_rate, loss_val, acc, F1))\n",
    "        # declay lr\n",
    "        if epoch % 3 == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * 0.9)\n",
    "    else:\n",
    "        print('\\t valid_loss %.4f, acc_valid %.3f, F1_valid %.3f, ' % (loss_val, acc, F1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid(data_iter_train, data_iter_valid, model, loss, trainer, ctx, nepochs,\n",
    "                penal_coeff=0.0, clip=None, class_weight=None, loss_name='sce'):\n",
    "\n",
    "    for epoch in range(1, nepochs+1):\n",
    "        start = time.time()\n",
    "        # train\n",
    "        is_train = True\n",
    "        one_epoch(data_iter_train, model, loss, trainer, ctx, is_train,\n",
    "                  epoch, penal_coeff, clip, class_weight, loss_name)\n",
    "\n",
    "        # valid\n",
    "        is_train = False\n",
    "        one_epoch(data_iter_valid, model, loss, trainer, ctx, is_train,\n",
    "                  epoch, penal_coeff, clip, class_weight, loss_name)\n",
    "        end = time.time()\n",
    "        print('time %.2f sec' % (end-start))\n",
    "        print(\"*\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Now that we are training the model, we use WeightedSoftmaxCE to alleviate the problem of data category imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = None\n",
    "loss_name = 'sce'\n",
    "optim = 'adam'\n",
    "lr = 0.001\n",
    "penal_coeff = 0.003\n",
    "clip = .5\n",
    "nepochs = 20\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), optim, {'learning_rate': lr})\n",
    "\n",
    "if loss_name == 'sce':\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "elif loss_name == 'wsce':\n",
    "    loss = WeightedSoftmaxCE()\n",
    "    # the value of class_weight is obtained by counting data in advance. It can be seen as a hyperparameter.\n",
    "    class_weight = nd.array([1., 3.], ctx=ctx)\n",
    "elif loss_name == 'l1':\n",
    "    loss = gluon.loss.L1Loss()\n",
    "elif loss_name == 'l2':\n",
    "    loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, learning_rate 0.00100 \n",
      "\t train_loss 0.6120, acc_train 0.729, F1_train 0.077, \n",
      "\t valid_loss 0.5957, acc_valid 0.746, F1_valid 0.066, \n",
      "time 33.94 sec\n",
      "****************************************************************************************************\n",
      "epoch 2, learning_rate 0.00100 \n",
      "\t train_loss 0.6195, acc_train 0.735, F1_train 0.123, \n",
      "\t valid_loss 0.5966, acc_valid 0.725, F1_valid 0.149, \n",
      "time 34.12 sec\n",
      "****************************************************************************************************\n",
      "epoch 3, learning_rate 0.00100 \n",
      "\t train_loss 0.5756, acc_train 0.746, F1_train 0.193, \n",
      "\t valid_loss 0.5586, acc_valid 0.751, F1_valid 0.210, \n",
      "time 34.07 sec\n",
      "****************************************************************************************************\n",
      "epoch 4, learning_rate 0.00090 \n",
      "\t train_loss 0.5790, acc_train 0.748, F1_train 0.241, \n",
      "\t valid_loss 0.6213, acc_valid 0.753, F1_valid 0.115, \n",
      "time 34.09 sec\n",
      "****************************************************************************************************\n",
      "epoch 5, learning_rate 0.00090 \n",
      "\t train_loss 0.5809, acc_train 0.749, F1_train 0.293, \n",
      "\t valid_loss 0.6437, acc_valid 0.752, F1_valid 0.046, \n",
      "time 34.55 sec\n",
      "****************************************************************************************************\n",
      "epoch 6, learning_rate 0.00090 \n",
      "\t train_loss 0.5785, acc_train 0.761, F1_train 0.274, \n",
      "\t valid_loss 0.5699, acc_valid 0.753, F1_valid 0.206, \n",
      "time 34.02 sec\n",
      "****************************************************************************************************\n",
      "epoch 7, learning_rate 0.00081 \n",
      "\t train_loss 0.5615, acc_train 0.751, F1_train 0.231, \n",
      "\t valid_loss 0.5858, acc_valid 0.741, F1_valid 0.254, \n",
      "time 34.67 sec\n",
      "****************************************************************************************************\n",
      "epoch 8, learning_rate 0.00081 \n",
      "\t train_loss 0.5467, acc_train 0.764, F1_train 0.345, \n",
      "\t valid_loss 0.5890, acc_valid 0.727, F1_valid 0.128, \n",
      "time 34.57 sec\n",
      "****************************************************************************************************\n",
      "epoch 9, learning_rate 0.00081 \n",
      "\t train_loss 0.5406, acc_train 0.771, F1_train 0.392, \n",
      "\t valid_loss 0.5830, acc_valid 0.734, F1_valid 0.222, \n",
      "time 34.46 sec\n",
      "****************************************************************************************************\n",
      "epoch 10, learning_rate 0.00073 \n",
      "\t train_loss 0.5133, acc_train 0.787, F1_train 0.473, \n",
      "\t valid_loss 0.5650, acc_valid 0.737, F1_valid 0.220, \n",
      "time 34.52 sec\n",
      "****************************************************************************************************\n",
      "epoch 11, learning_rate 0.00073 \n",
      "\t train_loss 0.4966, acc_train 0.799, F1_train 0.513, \n",
      "\t valid_loss 0.5844, acc_valid 0.746, F1_valid 0.310, \n",
      "time 34.84 sec\n",
      "****************************************************************************************************\n",
      "epoch 12, learning_rate 0.00073 \n",
      "\t train_loss 0.4753, acc_train 0.811, F1_train 0.563, \n",
      "\t valid_loss 0.6058, acc_valid 0.722, F1_valid 0.276, \n",
      "time 34.41 sec\n",
      "****************************************************************************************************\n",
      "epoch 13, learning_rate 0.00066 \n",
      "\t train_loss 0.4453, acc_train 0.833, F1_train 0.631, \n",
      "\t valid_loss 0.6542, acc_valid 0.710, F1_valid 0.341, \n",
      "time 34.41 sec\n",
      "****************************************************************************************************\n",
      "epoch 14, learning_rate 0.00066 \n",
      "\t train_loss 0.4304, acc_train 0.845, F1_train 0.657, \n",
      "\t valid_loss 0.6511, acc_valid 0.706, F1_valid 0.326, \n",
      "time 34.92 sec\n",
      "****************************************************************************************************\n",
      "epoch 15, learning_rate 0.00066 \n",
      "\t train_loss 0.4112, acc_train 0.853, F1_train 0.685, \n",
      "\t valid_loss 0.6573, acc_valid 0.708, F1_valid 0.360, \n",
      "time 34.71 sec\n",
      "****************************************************************************************************\n",
      "epoch 16, learning_rate 0.00059 \n",
      "\t train_loss 0.3810, acc_train 0.876, F1_train 0.736, \n",
      "\t valid_loss 0.6794, acc_valid 0.723, F1_valid 0.394, \n",
      "time 34.51 sec\n",
      "****************************************************************************************************\n",
      "epoch 17, learning_rate 0.00059 \n",
      "\t train_loss 0.3684, acc_train 0.884, F1_train 0.755, \n",
      "\t valid_loss 0.6842, acc_valid 0.695, F1_valid 0.308, \n",
      "time 34.51 sec\n",
      "****************************************************************************************************\n",
      "epoch 18, learning_rate 0.00059 \n",
      "\t train_loss 0.3526, acc_train 0.892, F1_train 0.774, \n",
      "\t valid_loss 0.6956, acc_valid 0.706, F1_valid 0.329, \n",
      "time 34.24 sec\n",
      "****************************************************************************************************\n",
      "epoch 19, learning_rate 0.00053 \n",
      "\t train_loss 0.3456, acc_train 0.900, F1_train 0.786, \n",
      "\t valid_loss 0.7350, acc_valid 0.693, F1_valid 0.382, \n",
      "time 34.54 sec\n",
      "****************************************************************************************************\n",
      "epoch 20, learning_rate 0.00053 \n",
      "\t train_loss 0.3554, acc_train 0.893, F1_train 0.776, \n",
      "\t valid_loss 0.6868, acc_valid 0.714, F1_valid 0.312, \n",
      "time 34.48 sec\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# train and valid\n",
    "train_valid(train_dataloader, valid_dataloader, model, loss, \\\n",
    "            trainer, ctx, nepochs, penal_coeff=penal_coeff, \\\n",
    "            clip=clip, class_weight=class_weight, loss_name=loss_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "Now we will randomly input a movie plot summary and its review to see if the model decide the review is a spoiler or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Parameter 'embedding0_weight' is missing in file 'model/att-0001.params', which contains parameters: 'selfattentivebilstm2_selfattentivebilstm12_embedding0_weight', 'selfattentivebilstm2_selfattentivebilstm12_lstm0_l0_i2h_weight', 'selfattentivebilstm2_selfattentivebilstm12_lstm0_l0_h2h_weight', ..., 'selfattentivebilstm2_selfattentivebilstm12_selfattention0_dense1_weight', 'selfattentivebilstm2_selfattentivebilstm12_selfattention0_dense1_bias', 'selfattentivebilstm2_selfattentivebilstm12_dense0_weight', 'selfattentivebilstm2_selfattentivebilstm12_dense0_bias'. Please make sure source and target networks have the same prefix.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-d635b9d25ffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model/att-0001.params'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m '''\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/gluon/block.py\u001b[0m in \u001b[0;36mload_parameters\u001b[0;34m(self, filename, ctx, allow_missing, ignore_extra)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mloaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             self.collect_params().load(\n\u001b[0;32m--> 386\u001b[0;31m                 filename, ctx, allow_missing, ignore_extra, self.prefix)\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/gluon/parameter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, filename, ctx, allow_missing, ignore_extra, restore_prefix)\u001b[0m\n\u001b[1;32m    909\u001b[0m                     \u001b[0;34m\"Parameter '%s' is missing in file '%s', which contains parameters: %s. \"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m                     \"Please make sure source and target networks have the same prefix.\"%(\n\u001b[0;32m--> 911\u001b[0;31m                         name[lprefix:], filename, _brief_print_list(arg_dict.keys()))\n\u001b[0m\u001b[1;32m    912\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Parameter 'embedding0_weight' is missing in file 'model/att-0001.params', which contains parameters: 'selfattentivebilstm2_selfattentivebilstm12_embedding0_weight', 'selfattentivebilstm2_selfattentivebilstm12_lstm0_l0_i2h_weight', 'selfattentivebilstm2_selfattentivebilstm12_lstm0_l0_h2h_weight', ..., 'selfattentivebilstm2_selfattentivebilstm12_selfattention0_dense1_weight', 'selfattentivebilstm2_selfattentivebilstm12_selfattention0_dense1_bias', 'selfattentivebilstm2_selfattentivebilstm12_dense0_weight', 'selfattentivebilstm2_selfattentivebilstm12_dense0_bias'. Please make sure source and target networks have the same prefix."
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    model = gluon.nn.SymbolBlock.imports(\"model/att-symbol.json\", ['data'], \\\n",
    "                                         \"model/att-0001.params\", ctx=ctx)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = 'john is a former police, his daughter, sarah was killed in a car accident, which he \\\n",
    "        did not think was purely accidental. with his investigation went deeper, he found \\\n",
    "        that sarah\\'s boyfriend, jack, was the one to blame.'\n",
    "left = 'Word embedding can effectively represent the semantic similarity between words, \\\n",
    "        which brings many breakthroughs for natural language processing tasks. \\\n",
    "        The attention mechanism can intuitively grasp the important semantic features \\\n",
    "        in the sentence. I liked sarah and jack in this movie, but hate john, because he \\\n",
    "        killed sarah'\n",
    "left_token = vocab[tokenizer(left)]\n",
    "right_token = vocab[tokenizer(right)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[ 0.28886735 -0.08389135]]\n",
       " <NDArray 1x2 @gpu(0)>, (1, 20, 56), (1, 20, 49))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_input = nd.array(left_token, ctx=ctx).reshape(1,-1)\n",
    "right_input = nd.array(right_token, ctx=ctx).reshape(1,-1)\n",
    "pred, att_left, att_right = model(left_input, right_input)\n",
    "pred, att_left.shape, att_right.shape\n",
    "#model.embedding_layer_right.weight.list_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to intuitively feel the role of the attention mechanism, we visualize the output of the model's attention on the predicted samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x144 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "np.squeeze(att_left.asnumpy(), 0).shape\n",
    "plt.figure(figsize=(40,2))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(np.squeeze(att_left.sum(axis=1, keepdims=True).asnumpy(), 0), cmap=cmap, annot=True,\n",
    "            xticklabels=tokenizer(left), yticklabels=['att_all'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Word embedding can effectively represent the semantic similarity between words, which brings many breakthroughs for complex natural language processing tasks. The attention mechanism can intuitively grasp the important semantic features in the sentence. The LSTM captures the word order relationship between words in a sentence. Through word embedding, LSTM and attention mechanisms work together to effectively represent the semantics of a sentence and apply it to many practical tasks.\n",
    "\n",
    "GluonNLP provides us with many efficient and convenient tools to help us experiment quickly. This greatly simplifies the tedious work of natural language processing.\n",
    "\n",
    "## Reference\n",
    "1. [A Structured Self-Attentive Sentence Embedding](https://arxiv.org/pdf/1703.03130.pdf)\n",
    "2. [Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "3. [Long short-term memory](https://www.bioinf.jku.at/publications/older/2604.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
